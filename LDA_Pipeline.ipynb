{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# LDA Pipeline for Contracts\n",
    "\n",
    "Generates a topic-by-topic graph (K=16) comparing mean of obligation_constraint with mean of permission_entitlement for employers, and then another set of graphs comparing this mean for workers"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import gensim\n",
    "#from contracts_global import doc2tokens, batchStreamObjectBranches\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import pipeline\n",
    "import plutil"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Part 0: Initialize the pipeline"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for config file ./configs/canadian_jj.conf\n",
      "Full output path: C:/Dropbox/Labor_Contracts_Canada/./analysis/output/analysis/2022-12-jeff-python/\n",
      "Looking for config file ./configs/canadian_jj.conf\n"
     ]
    }
   ],
   "source": [
    "pl = pipeline.Pipeline(\"canadian\", config_fname=\"canadian_jj.conf\", mode=\"s3\",\n",
    "                       lang_list=[\"eng\"], splitter=\"elliott\", batch_mode=\"contract\",\n",
    "                       use_aws=False, num_lda_topics=20, verbose=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Part 1: Prepare LDA inputs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 1: Extract object branches"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pdata files from C:/Dropbox/Labor_Contracts_Canada/./analysis/output/analysis/2022-12-jeff-python/03b_pdata_pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0003405a_eng.pkl: 100%|██████████| 20/20 [00:00<00:00, 48.25it/s]\n"
     ]
    }
   ],
   "source": [
    "# This will hold DFs for each contract, which we combine at the end to make a\n",
    "# single obranch.pkl file\n",
    "all_obranch_dfs = []\n",
    "pdata_pkl_path = pl.get_pdata_output_path()\n",
    "pl.vprint(\"Loading pdata files from \" + str(pdata_pkl_path))\n",
    "pdata_pkl_fpaths = glob.glob(os.path.join(pdata_pkl_path,\"*.pkl\"))\n",
    "pdata_iter = tqdm(pdata_pkl_fpaths)\n",
    "for cur_pdata_pkl_fpath in pdata_iter:\n",
    "    pkl_fname = os.path.basename(cur_pdata_pkl_fpath)\n",
    "    pdata_iter.set_description(pkl_fname)\n",
    "    pdata_df = plutil.safe_load_pickle(cur_pdata_pkl_fpath)\n",
    "    object_df = pdata_df[[\"contract_id\",\"article_num\",\"sentence_num\",\n",
    "                          \"statement_num\",\"object_branches\"]].copy()\n",
    "    num_obj_branches = len(object_df)\n",
    "    # Transforms the object_branches string into a single space-separated string\n",
    "    def parse_obj_branch(obj_branch_str):\n",
    "        all_words = [' '.join(word_list) for word_list in obj_branch_str]\n",
    "        all_words_str = ' '.join(all_words)\n",
    "        return all_words_str\n",
    "    # Convert the object_branches string into a Python list\n",
    "    object_df[\"obj_branch_doc\"] = object_df[\"object_branches\"].apply(parse_obj_branch)\n",
    "    # So now we don't need the list version anymore\n",
    "    del object_df[\"object_branches\"]\n",
    "    # But now we need to \"sum\" (concatenate) up to the article level\n",
    "    # First to the sentence level. agg(lambda col: \" \".join(col)) is crucial here\n",
    "    sent_df = object_df.groupby([\"contract_id\",\"article_num\",\"sentence_num\"], as_index=False)[\"obj_branch_doc\"].agg(\" \".join)\n",
    "    # And then the article level\n",
    "    art_df = sent_df.groupby([\"contract_id\",\"article_num\"], as_index=False)[\"obj_branch_doc\"].agg(\" \".join)\n",
    "    # Now we have a df of object branches at the article level!\n",
    "    all_obranch_dfs.append(art_df)\n",
    "# Now we combine the obranch dfs into a single DF across all contracts\n",
    "obranch_df = pd.concat(all_obranch_dfs)\n",
    "obranch_pkl_fpath = pl.get_obranch_fpath(ext=\"pkl\")\n",
    "obranch_csv_fpath = pl.get_obranch_fpath(ext=\"csv\")\n",
    "#pl.vprint(f\"Object branches parsed. Saving {obranch_pkl_fpath}\")\n",
    "plutil.safe_to_pickle(obranch_df, obranch_pkl_fpath)\n",
    "plutil.safe_to_csv(obranch_df, obranch_csv_fpath)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 2: Construct Gensim dictionary"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Uses gensim instead of spacy to run the topic model\n",
    "import codecs\n",
    "import csv\n",
    "import os\n",
    "from six import iteritems\n",
    "import string\n",
    "\n",
    "import gensim\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import plutil"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# Preprocessing options\n",
    "# (Set to -1 to not remove any tokens)\n",
    "remove_top_n = -1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 423/423 [00:00<00:00, 444.31it/s]\n"
     ]
    }
   ],
   "source": [
    "# Initialize the dictionary\n",
    "contract_dict = gensim.corpora.Dictionary()\n",
    "# We can just directly load the .pkl file of the exported df from\n",
    "# extract_object_branches\n",
    "obranch_fpath = pl.get_obranch_fpath(ext=\"pkl\")\n",
    "obranch_df = plutil.safe_load_pickle(obranch_fpath)\n",
    "article_stream = obranch_df[\"obj_branch_doc\"]\n",
    "\n",
    "# Now use this generator to add all the documents with preprocessing\n",
    "# We also want to save the preprocessed articles (so we don't have to run\n",
    "# through and preprocess them twice) so we accumulate them into a big list\n",
    "preprocessed_articles = []\n",
    "def stream_preprocessed():\n",
    "    c = 0\n",
    "    for cur_article in tqdm(article_stream):\n",
    "        article_clean = pl.preprocess_text(cur_article)\n",
    "        preprocessed_articles.append(article_clean)\n",
    "        yield article_clean\n",
    "contract_dict.add_documents(stream_preprocessed())\n",
    "# Final preprocessing step: remove tokens that appear in too many contracts\n",
    "if remove_top_n > -1:\n",
    "    contract_dict.filter_n_most_frequent(remove_top_n)\n",
    "dict_fpath = pl.get_lda_dict_fpath()\n",
    "# Make sure the dirs exist before saving\n",
    "plutil.make_dirs(dict_fpath)\n",
    "contract_dict.save(dict_fpath)\n",
    "# And save the list of preprocessed docs as well\n",
    "joblib.dump(preprocessed_articles, pl.get_preprocessed_fpath())\n",
    "# AND if we're working with the object branches, add it as a column in the\n",
    "# original object branch df and save the new version as well\n",
    "obranch_df[\"doc_clean\"] = preprocessed_articles\n",
    "plutil.safe_to_pickle(obranch_df, pl.get_preprocessed_df_fpath())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "   contract_id  article_num  \\\n0     0000102a            0   \n1     0000102a            1   \n2     0000102a            2   \n3     0000102a            3   \n4     0000102a            4   \n..         ...          ...   \n21    0003405a           21   \n22    0003405a           23   \n23    0003405a           24   \n24    0003405a           25   \n25    0003405a           26   \n\n                                       obj_branch_doc  \\\n0   the general purpose of this agreement to set f...   \n1   into force on the date of its by the party sha...   \n2   in wood operation in t h e province of newfoun...   \n3   for the purpose of erect structure outside the...   \n4   s  h agreement  a t i t h e procurement of woo...   \n..                                                ...   \n21  a vision care plan which will provide for expe...   \n22  applicable for the aforesaid employee where be...   \n23                 to become eligible under this plan   \n24  if be an authorized leave of absence have be u...   \n25  a person who be employ in a full time regular ...   \n\n                                            doc_clean  \n0   [purpos, agreement, set, forth, work, condit, ...  \n1   [forc, date, parti, shall, remain, forc, effec...  \n2   [wood, oper, provinc, newfoundland, save, wood...  \n3   [purpos, erect, structur, outsid, limit, compa...  \n4   [agreement, procur, wood, manufactur, comparis...  \n..                                                ...  \n21  [vision, care, plan, provid, expens, incur, em...  \n22  [applic, aforesaid, employe, collect, agreemen...  \n23                                       [elig, plan]  \n24  [author, leav, absenc, term, plan, employe, co...  \n25  [person, emploi, time, regular, posit, includ,...  \n\n[423 rows x 4 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>contract_id</th>\n      <th>article_num</th>\n      <th>obj_branch_doc</th>\n      <th>doc_clean</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0000102a</td>\n      <td>0</td>\n      <td>the general purpose of this agreement to set f...</td>\n      <td>[purpos, agreement, set, forth, work, condit, ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0000102a</td>\n      <td>1</td>\n      <td>into force on the date of its by the party sha...</td>\n      <td>[forc, date, parti, shall, remain, forc, effec...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0000102a</td>\n      <td>2</td>\n      <td>in wood operation in t h e province of newfoun...</td>\n      <td>[wood, oper, provinc, newfoundland, save, wood...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0000102a</td>\n      <td>3</td>\n      <td>for the purpose of erect structure outside the...</td>\n      <td>[purpos, erect, structur, outsid, limit, compa...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0000102a</td>\n      <td>4</td>\n      <td>s  h agreement  a t i t h e procurement of woo...</td>\n      <td>[agreement, procur, wood, manufactur, comparis...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>0003405a</td>\n      <td>21</td>\n      <td>a vision care plan which will provide for expe...</td>\n      <td>[vision, care, plan, provid, expens, incur, em...</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>0003405a</td>\n      <td>23</td>\n      <td>applicable for the aforesaid employee where be...</td>\n      <td>[applic, aforesaid, employe, collect, agreemen...</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>0003405a</td>\n      <td>24</td>\n      <td>to become eligible under this plan</td>\n      <td>[elig, plan]</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>0003405a</td>\n      <td>25</td>\n      <td>if be an authorized leave of absence have be u...</td>\n      <td>[author, leav, absenc, term, plan, employe, co...</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>0003405a</td>\n      <td>26</td>\n      <td>a person who be employ in a full time regular ...</td>\n      <td>[person, emploi, time, regular, posit, includ,...</td>\n    </tr>\n  </tbody>\n</table>\n<p>423 rows × 4 columns</p>\n</div>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obranch_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Part 2: AWS processing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 3: Split the object branch docs into chunks"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "# Split the full set of docs up into smaller chunks, so corpora can be constructed\n",
    "# for each chunk (on different AWS instances) and then combined at the end\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "\n",
    "#import aws_api\n",
    "import plutil"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [],
   "source": [
    "def txt_to_doc(cur_txt, dictionary):\n",
    "    # Takes in a string containing the plaintext document, and outputs a gensim\n",
    "    # format doc\n",
    "    return dictionary.doc2bow(cur_txt.lower().split())\n",
    "\n",
    "def split_and_save(art_df):\n",
    "    if pl.get_num_lda_chunks() == -1:\n",
    "        # No splitting required, just save the df\n",
    "        output_fpath = pl.get_lda_doclist_fpath()\n",
    "        plutil.safe_to_pickle(art_df, output_fpath)\n",
    "        pl.vprint(f\"Saved {output_fpath}\")\n",
    "        return [output_fpath]\n",
    "    # Otherwise, if we're here, split into a list of dfs, one for each subcorpus,\n",
    "    # with num_lda_subsets subcorpora total\n",
    "    fpath_list = []\n",
    "    df_list = np.array_split(art_df, pl.get_num_lda_chunks())\n",
    "    df_lens = [len(df) for df in df_list]\n",
    "    pl.vprint(\"DF split into \" + str(pl.num_lda_chunks) + \" pieces, lengths: \" + str(df_lens))\n",
    "    # Finally, save each one to a separate pickle file\n",
    "    for doclist_num, cur_doclist in enumerate(df_list):\n",
    "        output_fpath = pl.get_lda_doclist_fpath(doclist_num)\n",
    "        plutil.safe_to_pickle(cur_doclist, output_fpath)\n",
    "        pl.vprint(\"Saved \" + str(output_fpath))\n",
    "        fpath_list.append(output_fpath)\n",
    "    return fpath_list\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split_docs()\n",
      "Loading C:/Dropbox/Labor_Contracts_Canada/./analysis/output/analysis/2022-12-jeff-python/lda02c_preprocessed_df.pkl\n",
      "Saved C:/Dropbox/Labor_Contracts_Canada/./analysis/output/analysis/2022-12-jeff-python/lda03_doclist.pkl\n"
     ]
    }
   ],
   "source": [
    "pl.vprint(\"split_docs()\")\n",
    "# New version: I'm just going to load all the object branches, tokenize them,\n",
    "# and pickle them\n",
    "# The extra float('inf') element is just to make the loop simpler\n",
    "#if not pl.num_lda_subsets:\n",
    "#    raise Exception(\"num_lda_subsets must be specified in the Pipeline \"\n",
    "#                    + \"constructor if you are running the LDA pipeline\")\n",
    "# It's a bit easier in object branch mode, since we just split a big df\n",
    "obranch_df_fpath = pl.get_preprocessed_df_fpath()\n",
    "pl.vprint(\"Loading \" + str(obranch_df_fpath))\n",
    "obranch_df = plutil.safe_load_pickle(obranch_df_fpath)\n",
    "# And just split this bad boi\n",
    "obranch_fpath_list = split_and_save(obranch_df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [
    {
     "data": {
      "text/plain": "   contract_id  article_num  \\\n0     0000102a            0   \n1     0000102a            1   \n2     0000102a            2   \n3     0000102a            3   \n4     0000102a            4   \n..         ...          ...   \n21    0003405a           21   \n22    0003405a           23   \n23    0003405a           24   \n24    0003405a           25   \n25    0003405a           26   \n\n                                       obj_branch_doc  \\\n0   the general purpose of this agreement to set f...   \n1   into force on the date of its by the party sha...   \n2   in wood operation in t h e province of newfoun...   \n3   for the purpose of erect structure outside the...   \n4   s  h agreement  a t i t h e procurement of woo...   \n..                                                ...   \n21  a vision care plan which will provide for expe...   \n22  applicable for the aforesaid employee where be...   \n23                 to become eligible under this plan   \n24  if be an authorized leave of absence have be u...   \n25  a person who be employ in a full time regular ...   \n\n                                            doc_clean  \n0   [purpos, agreement, set, forth, work, condit, ...  \n1   [forc, date, parti, shall, remain, forc, effec...  \n2   [wood, oper, provinc, newfoundland, save, wood...  \n3   [purpos, erect, structur, outsid, limit, compa...  \n4   [agreement, procur, wood, manufactur, comparis...  \n..                                                ...  \n21  [vision, care, plan, provid, expens, incur, em...  \n22  [applic, aforesaid, employe, collect, agreemen...  \n23                                       [elig, plan]  \n24  [author, leav, absenc, term, plan, employe, co...  \n25  [person, emploi, time, regular, posit, includ,...  \n\n[423 rows x 4 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>contract_id</th>\n      <th>article_num</th>\n      <th>obj_branch_doc</th>\n      <th>doc_clean</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0000102a</td>\n      <td>0</td>\n      <td>the general purpose of this agreement to set f...</td>\n      <td>[purpos, agreement, set, forth, work, condit, ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0000102a</td>\n      <td>1</td>\n      <td>into force on the date of its by the party sha...</td>\n      <td>[forc, date, parti, shall, remain, forc, effec...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0000102a</td>\n      <td>2</td>\n      <td>in wood operation in t h e province of newfoun...</td>\n      <td>[wood, oper, provinc, newfoundland, save, wood...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0000102a</td>\n      <td>3</td>\n      <td>for the purpose of erect structure outside the...</td>\n      <td>[purpos, erect, structur, outsid, limit, compa...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0000102a</td>\n      <td>4</td>\n      <td>s  h agreement  a t i t h e procurement of woo...</td>\n      <td>[agreement, procur, wood, manufactur, comparis...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>0003405a</td>\n      <td>21</td>\n      <td>a vision care plan which will provide for expe...</td>\n      <td>[vision, care, plan, provid, expens, incur, em...</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>0003405a</td>\n      <td>23</td>\n      <td>applicable for the aforesaid employee where be...</td>\n      <td>[applic, aforesaid, employe, collect, agreemen...</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>0003405a</td>\n      <td>24</td>\n      <td>to become eligible under this plan</td>\n      <td>[elig, plan]</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>0003405a</td>\n      <td>25</td>\n      <td>if be an authorized leave of absence have be u...</td>\n      <td>[author, leav, absenc, term, plan, employe, co...</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>0003405a</td>\n      <td>26</td>\n      <td>a person who be employ in a full time regular ...</td>\n      <td>[person, emploi, time, regular, posit, includ,...</td>\n    </tr>\n  </tbody>\n</table>\n<p>423 rows × 4 columns</p>\n</div>"
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obranch_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 4: Export to AWS for batch processing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [],
   "source": [
    "# Need to install paramiko and fabric if you're using AWS\n",
    "#!pip install paramiko\n",
    "#!pip install fabric"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [],
   "source": [
    "# Exports the necessary files to the various AWS instances, before\n",
    "# remote_control_aws runs them\n",
    "import datetime\n",
    "import glob\n",
    "import os\n",
    "\n",
    "import aws_api\n",
    "import fabric_api"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [],
   "source": [
    "# SSH settings\n",
    "dns_template = \"DNS[{i}]=\\\"{the_dns}\\\"\"\n",
    "env_var_template = \"export DNS{i}=\\\"{dns}\\\"\"\n",
    "ssh_template = \"ssh -i /home/jjacobs/aws/jeff.pem -o StrictHostKeyChecking=no ubuntu@{dns} \\\"mkdir obranch_lda; mv 02* 03* obranch_lda; /home/ubuntu/anaconda3/bin/conda install -y -q fabric; /home/ubuntu/anaconda3/bin/python canadian_aws_corpus_run.py {inst_num};\\\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "outputs": [],
   "source": [
    "def export_to_aws(custom_glob=None, instances=None):\n",
    "    # Set to True if you want to export the files via shell script rather than\n",
    "    # within Python\n",
    "    create_shell_script = False\n",
    "    if custom_glob:\n",
    "        pl.iprint(\"Using custom glob: \\\"\" + custom_glob + \"\\\"\")\n",
    "    # First we have to launch the aws instances\n",
    "    inst_ids = aws_api.start_instances(pl.num_lda_subsets)\n",
    "    # Get their DNS addresses\n",
    "    dns_list = aws_api.get_all_dns()\n",
    "    if create_shell_script:\n",
    "        # Print the DNS array for the aws_corpus_export.sh file\n",
    "        for dns_num, cur_dns in enumerate(dns_list):\n",
    "            print(dns_template.format(i=dns_num, the_dns=cur_dns))\n",
    "        # Now print the export commands that will store the DNS names\n",
    "        for dns_num, cur_dns in enumerate(dns_list):\n",
    "            print(env_var_template.format(i=dns_num, dns=cur_dns))\n",
    "        # Print the ssh commands for the ssh_run_aws.sh file\n",
    "        ssh_list = []\n",
    "        for dns_num, cur_dns in enumerate(dns_list):\n",
    "            ssh_list.append(\"( \" + ssh_template.format(dns=cur_dns, inst_num=dns_num) + \" ) & \")\n",
    "        # Ugh\n",
    "        ssh_str = \"\".join(ssh_list)\n",
    "        print(ssh_str)\n",
    "    else:\n",
    "        # Do the exporting directly through python\n",
    "        fpath_list = []\n",
    "        # Standard set of files\n",
    "        fpath_list.append(os.path.join(\"configs\",\"canadian_dominik.conf\"))\n",
    "        lda_fpath_list = glob.glob(os.path.join(\"lda_pipeline\",\"lda*\"))\n",
    "        fpath_list.extend(lda_fpath_list)\n",
    "        fpath_list.append(\"pipeline.py\")\n",
    "        fpath_list.append(\"pipeline_util.py\")\n",
    "        fpath_list.append(\"canadian_aws_corpus_run.py\")\n",
    "        fpath_list.append(\"aws_api.py\")\n",
    "        fpath_list.append(\"fabric_api.py\")\n",
    "        #fpath_list.append(pl.get_lda_dict_fpath())\n",
    "        # And the actual exporting\n",
    "        for dns_num, cur_dns in enumerate(dns_list):\n",
    "            if dns_num not in instances:\n",
    "                continue\n",
    "            print(\"Copying to instance #\" + str(dns_num))\n",
    "            # Need to make sure to copy the specific doclist file\n",
    "            cur_fpath_list = fpath_list.copy()\n",
    "            #cur_fpath_list.append(pl.get_lda_doclist_fpath(dns_num))\n",
    "            fabric_api.copy_to_instance(cur_fpath_list, cur_dns)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 5: Remote control the AWS instances"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "outputs": [],
   "source": [
    "# Use Fabric to run construct_corpus on AWS remotely\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "import plutil\n",
    "import aws_api\n",
    "import fabric_api"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [],
   "source": [
    "if pl.use_aws:\n",
    "    dns_list = aws_api.get_all_dns()\n",
    "    # Run the python file remotely on each instance\n",
    "    commands = [\n",
    "        \"mkdir lda\",\n",
    "        \"mv 02* 03* lda\",\n",
    "        \"/home/ubuntu/anaconda3/bin/python canadian_aws_corpus_run.py $INST_NUM\",\n",
    "        #\"scp obranch_lda/05* jjacobs@textlab.econ.columbia.edu:~/ashmacnaidu/canadian_data/obranch_lda/\"\n",
    "    ]\n",
    "    result_dict = fabric_api.run_commands(dns_list, commands)\n",
    "    print(\"DNS info:\")\n",
    "    print([(dns_num, cur_dns) for dns_num, cur_dns in enumerate(dns_list)])\n",
    "    timestamp = str(datetime.datetime.now()).replace(\" \",\"_\").split(\".\")[0]\n",
    "    result_fpath = os.path.join(\".\",\"logs\",\"aws_output_\" + timestamp + \".pkl\")\n",
    "    print(\"Saving result_dict to \" + result_fpath)\n",
    "    plutil.safe_to_pickle(result_dict, result_fpath)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 6: Construct the LDA corpus\n",
    "\n",
    "(The key step, necessary whether or not you're running on AWS)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [],
   "source": [
    "# Uses joblib to parallelize the construction of the corpora.\n",
    "import itertools\n",
    "import logging\n",
    "import multiprocessing\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "import joblib\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import plutil"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "outputs": [],
   "source": [
    "class CanadianCorpus(object):\n",
    "    def __init__(self, dictionary):\n",
    "        self.dictionary = dictionary\n",
    "\n",
    "    def __iter__(self):\n",
    "        return (self.dictionary.doc2bow(f.lower().split()) for f in stream_object_branches())\n",
    "\n",
    "def construct_streaming_corpus(pl, dictionary):\n",
    "    # Sets up the corpus and dictionary for topic modeling\n",
    "    #print(\"constructCorpus()\")\n",
    "    corpus = CanadianCorpus(dictionary)\n",
    "    #corpus = CanadianObjectBranchCorpus(dictionary)\n",
    "    return corpus\n",
    "\n",
    "def load_doclist_df(doclist_num):\n",
    "    # Loads the doc list created by serialize_docs(). On TL this will be in the\n",
    "    # lda subfolder, whereas on AWS it will just be in the working directory\n",
    "    serialized_fpath = pl.get_lda_doclist_fpath(chunk_num=doclist_num)\n",
    "    doc_df = plutil.safe_load_pickle(serialized_fpath)\n",
    "    return doc_df\n",
    "\n",
    "def save_static_corpus(subcorpus, subcorp_num):\n",
    "    # Save the *actual* subcorpora (i.e., we're done with the doclists now)\n",
    "    # Using current directory since this will be run on AWS\n",
    "    if subcorp_num >= 0:\n",
    "        pl.vprint(\"Saving subcorpus #\" + str(subcorp_num))\n",
    "        static_fpath = pl.get_lda_subcorp_fpath(subcorp_num)\n",
    "    else:\n",
    "        # Full corpus\n",
    "        pl.vprint(\"Saving full corpus\")\n",
    "        static_fpath = pl.get_lda_corpus_fpath()\n",
    "    plutil.make_dirs(static_fpath)\n",
    "    gensim.corpora.MmCorpus.serialize(static_fpath, subcorpus)\n",
    "    pl.vprint(f\"Corpus serialized to {static_fpath}\")\n",
    "\n",
    "def stream_to_static_corpus(pl, dictionary):\n",
    "    # Loads all of the object branches using streamObjectBranches but saves\n",
    "    # them into a static (serializable, non-streaming) corpus\n",
    "    corpus = [dictionary.doc2bow(f.lower().split()) for f in pl.stream_object_branches()]\n",
    "    return corpus\n",
    "\n",
    "def vectorize_docs(dictionary, cur_doc_list):\n",
    "    # Converts the list of documents from strings to bag-of-words vectors\n",
    "    return [dictionary.doc2bow(l) for l in cur_doc_list]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading doclist_df from C:/Dropbox/Labor_Contracts_Canada/./analysis/output/analysis/2022-12-jeff-python/lda03_doclist.pkl\n",
      "Dictionary loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=15)]: Using backend LokyBackend with 15 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus construction complete.\n",
      "Saving full corpus\n",
      "Corpus serialized to C:/Dropbox/Labor_Contracts_Canada/./analysis/output/analysis/2022-12-jeff-python/lda06_gensim_corpus.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=15)]: Done   1 out of   1 | elapsed:    0.4s finished\n"
     ]
    }
   ],
   "source": [
    "# [NOTE: Used to be construct_static_corpus(), until 2019-02-19 -JJ]\n",
    "#def construct_corpus(num_cores=8):\n",
    "# Construct the static corpus in parallel, to make the weight computations\n",
    "# quicker\n",
    "subcorp_num = -1\n",
    "if pl.use_aws:\n",
    "    # Get the subcorpus number for this AWS/TL instance\n",
    "    subcorp_num = pl.get_instance_num()\n",
    "    # Load the doclist for this subcorpus number\n",
    "    doclist_df = load_doclist_df(subcorp_num)\n",
    "else:\n",
    "    doclist_df_fpath = pl.get_lda_doclist_fpath()\n",
    "    pl.vprint(f\"Loading doclist_df from {doclist_df_fpath}\")\n",
    "    doclist_df = plutil.safe_load_pickle(doclist_df_fpath)\n",
    "doc_list = list(doclist_df[\"doc_clean\"])\n",
    "\n",
    "# Now load the dictionary so we have the doc2bow function\n",
    "dictionary = joblib.load(pl.get_lda_dict_fpath())\n",
    "pl.vprint(\"Dictionary loaded\")\n",
    "\n",
    "# Use multiprocessing to process batches of docs in parallel\n",
    "batch_size = 1000\n",
    "doc_chunks = [doc_list[i:i+batch_size] for i in range(0,len(doc_list),batch_size)]\n",
    "num_workers = plutil.get_num_workers()\n",
    "par_obj = Parallel(n_jobs=num_workers, verbose=5)\n",
    "chunk_vectors = par_obj(delayed(vectorize_docs)(dictionary, cur_chunk) for cur_chunk in doc_chunks)\n",
    "# Now put the chunks together and save in mm format\n",
    "all_docs = []\n",
    "for chunk in chunk_vectors:\n",
    "    all_docs.extend(chunk)\n",
    "pl.vprint(\"Corpus construction complete.\")\n",
    "save_static_corpus(all_docs, subcorp_num)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 7: Combine AWS corpora"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "outputs": [],
   "source": [
    "# Takes the separate corpora created on the various AWS instances and combines\n",
    "# them into one final corpus for use by the LDA model in the next step\n",
    "import itertools\n",
    "\n",
    "import gensim\n",
    "\n",
    "def load_subcorp(pl, subcorp_num):\n",
    "    subcorp_fpath = pl.get_lda_subcorp_fpath(subcorp_num=subcorp_num)\n",
    "    pl.iprint(\"Loading subcorpus #\" + str(subcorp_num) + \" from \" + str(subcorp_fpath))\n",
    "    subcorp = gensim.corpora.MmCorpus(subcorp_fpath)\n",
    "    return subcorp\n",
    "\n",
    "def combine_corpora(pl):\n",
    "    # See http://thread.gmane.org/gmane.comp.ai.gensim/1842\n",
    "    # (after a longer-than-needed search :( )\n",
    "    corpora = [load_subcorp(pl, i) for i in range(pl.num_lda_subsets)]\n",
    "    #gensim.corpora.MmCorpus.serialize(\"./canadian_obj_v2_corpus.mm\", itertools.chain(corpus0,corpus1))\n",
    "    # Note the * before corpora, so that each corpus gets passed in as an argument to the\n",
    "    # itertools.chain() function separately\n",
    "    gensim.corpora.MmCorpus.serialize(pl.get_lda_corpus_fpath(),\n",
    "                                      itertools.chain(*corpora))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 8: Run LDA"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "outputs": [],
   "source": [
    "# Trains an LDA model using gensim\n",
    "import logging\n",
    "import os\n",
    "\n",
    "import gensim\n",
    "import joblib\n",
    "\n",
    "import plutil"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "outputs": [],
   "source": [
    "def compute_lda_similarity(lda_model, doc1, doc2):\n",
    "    lda1 = lda_model[doc1]\n",
    "    lda2 = lda_model[doc2]\n",
    "    sim = gensim.matutils.cossim(lda1, lda2)\n",
    "    return sim\n",
    "\n",
    "def compute_sims(lda_model, dictionary):\n",
    "    # Loads in a list of pairs and computes similarities between the docs based\n",
    "    # on the topic proportions\n",
    "    pairs_df = pd.read_csv(pairs_file)\n",
    "    pairs_df[\"sim_lda\"] = np.nan\n",
    "    for row_num, row_data in pairs_df.iterrows():\n",
    "        debugPrint(\"*** Pair #\" + str(row_num))\n",
    "        debugPrint(row_data)\n",
    "        # Now get the corresponding docs and compute their LDA sims\n",
    "        contract_id = int(row_data[\"contract_id\"])\n",
    "        prev_id = int(row_data[\"prev_id\"])\n",
    "        cur_doc = txtToDoc(loadTxtById(contract_id), dictionary)\n",
    "        prev_doc = txtToDoc(loadTxtById(prev_id), dictionary)\n",
    "        sim = computeLDASimilarity(lda_model, cur_doc, prev_doc)\n",
    "        pairs_df.set_value(row_num, \"sim_lda\", sim)\n",
    "    return pairs_df\n",
    "\n",
    "def launch_lda(corpus, dictionary):\n",
    "    pl.vprint(\"Launching LDA model with \" + str(pl.num_lda_topics) + \" topics\")\n",
    "    print(corpus)\n",
    "    print(dictionary)\n",
    "    model = gensim.models.LdaMulticore(corpus, id2word=dictionary,\n",
    "                                       num_topics=pl.num_lda_topics,\n",
    "                                       workers=plutil.get_num_workers(),\n",
    "                                       iterations=100)\n",
    "    return model\n",
    "\n",
    "def load_lda(filename):\n",
    "    model = gensim.models.LdaMulticore.load(filename)\n",
    "    return model\n",
    "\n",
    "def pairwise_similarities(pl):\n",
    "    ## Compute similarities\n",
    "    sims = computeSims(model, canadian_dict)\n",
    "    saveSims(sims, sims_file)\n",
    "\n",
    "def print_lda(lda_model):\n",
    "    all_topics = lda_model.show_topics(num_topics=20,num_words=20)\n",
    "    for topic in all_topics:\n",
    "        print(topic[0],topic[1])\n",
    "        print(\"-----\")\n",
    "\n",
    "def save_lda(pl, model):\n",
    "    model.save(pl.get_lda_model_fpath())\n",
    "\n",
    "def save_sims(pairs_df, filename):\n",
    "    pairs_df.to_csv(filename,index=False)\n",
    "\n",
    "def similarity_test(lda_model):\n",
    "    ## Similarity test\n",
    "    dictionary = loadDictionary(os.path.join(LDA_PATH, CORPUS_NAME + \"_dict.pkl\"))\n",
    "    with codecs.open(\"/home/ubuntu/mongo_txts/00001_eng.txt\",'r','utf-8') as f:\n",
    "        cur_txt = f.read()\n",
    "        doc1 = dictionary.doc2bow(cur_txt.lower().split())\n",
    "    with codecs.open(\"/home/ubuntu/mongo_txts/00002_eng.txt\",'r','utf-8') as f:\n",
    "        cur_txt = f.read()\n",
    "        doc2 = dictionary.doc2bow(cur_txt.lower().split())\n",
    "    doc1_lda = lda_model[doc1]\n",
    "    doc2_lda = lda_model[doc2]\n",
    "    same_sim = gensim.matutils.cossim(doc1_lda,doc1_lda)\n",
    "    cur_sim = gensim.matutils.cossim(doc1_lda,doc2_lda)\n",
    "    print(same_sim)\n",
    "    print(cur_sim)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LDA dictionary from C:/Dropbox/Labor_Contracts_Canada/./analysis/output/analysis/2022-12-jeff-python/lda02a_gensim_dict.pkl\n",
      "Loading LDA corpus from C:/Dropbox/Labor_Contracts_Canada/./analysis/output/analysis/2022-12-jeff-python/lda06_gensim_corpus.pkl\n",
      "Launching LDA model with 20 topics\n",
      "MmCorpus(423 documents, 2977 features, 31956 non-zero entries)\n",
      "Dictionary<2977 unique tokens: ['abil', 'adjust', 'advanc', 'agreement', 'ail']...>\n",
      "0 0.039*\"classif\" + 0.035*\"agreement\" + 0.035*\"job\" + 0.031*\"chang\" + 0.026*\"rate\" + 0.022*\"schedul\" + 0.019*\"wage\" + 0.019*\"establish\" + 0.017*\"attach\" + 0.015*\"employe\" + 0.015*\"compani\" + 0.013*\"dai\" + 0.013*\"alter\" + 0.013*\"oper\" + 0.013*\"grievanc\" + 0.012*\"parti\" + 0.011*\"work\" + 0.010*\"method\" + 0.010*\"thirti\" + 0.010*\"advers\"\n",
      "-----\n",
      "1 0.048*\"dai\" + 0.037*\"work\" + 0.033*\"employe\" + 0.024*\"time\" + 0.021*\"holidai\" + 0.016*\"compani\" + 0.013*\"period\" + 0.012*\"senior\" + 0.011*\"job\" + 0.011*\"oper\" + 0.010*\"pai\" + 0.010*\"vacat\" + 0.009*\"agreement\" + 0.009*\"rate\" + 0.008*\"leav\" + 0.008*\"follow\" + 0.007*\"provid\" + 0.007*\"classif\" + 0.007*\"date\" + 0.007*\"earn\"\n",
      "-----\n",
      "2 0.032*\"holidai\" + 0.030*\"pai\" + 0.027*\"dai\" + 0.024*\"work\" + 0.024*\"employe\" + 0.019*\"time\" + 0.017*\"oper\" + 0.017*\"compani\" + 0.015*\"disabl\" + 0.013*\"agreement\" + 0.010*\"regular\" + 0.010*\"employ\" + 0.009*\"provid\" + 0.008*\"qualifi\" + 0.008*\"insur\" + 0.008*\"follow\" + 0.008*\"rate\" + 0.008*\"plan\" + 0.007*\"camp\" + 0.007*\"benefit\"\n",
      "-----\n",
      "3 0.026*\"employe\" + 0.017*\"work\" + 0.013*\"compani\" + 0.013*\"provid\" + 0.012*\"benefit\" + 0.011*\"employ\" + 0.011*\"agreement\" + 0.011*\"plan\" + 0.011*\"job\" + 0.011*\"period\" + 0.011*\"time\" + 0.010*\"disabl\" + 0.010*\"dai\" + 0.010*\"weekli\" + 0.010*\"month\" + 0.009*\"lai\" + 0.008*\"senior\" + 0.008*\"requir\" + 0.007*\"effect\" + 0.007*\"oper\"\n",
      "-----\n",
      "4 0.036*\"work\" + 0.023*\"employe\" + 0.018*\"provid\" + 0.018*\"requir\" + 0.018*\"transport\" + 0.018*\"board\" + 0.016*\"lodg\" + 0.013*\"marshal\" + 0.013*\"camp\" + 0.012*\"agreement\" + 0.012*\"time\" + 0.011*\"commun\" + 0.010*\"free\" + 0.010*\"commut\" + 0.009*\"hour\" + 0.008*\"road\" + 0.008*\"oper\" + 0.007*\"bunkhouseman\" + 0.007*\"perform\" + 0.007*\"job\"\n",
      "-----\n",
      "5 0.037*\"employe\" + 0.036*\"work\" + 0.022*\"dai\" + 0.021*\"employ\" + 0.018*\"compani\" + 0.016*\"oper\" + 0.013*\"hour\" + 0.012*\"camp\" + 0.011*\"time\" + 0.011*\"requir\" + 0.011*\"senior\" + 0.010*\"period\" + 0.009*\"regular\" + 0.009*\"agreement\" + 0.008*\"avail\" + 0.008*\"provid\" + 0.008*\"job\" + 0.008*\"condit\" + 0.007*\"rate\" + 0.007*\"list\"\n",
      "-----\n",
      "6 0.037*\"dai\" + 0.033*\"job\" + 0.028*\"work\" + 0.026*\"parti\" + 0.021*\"desir\" + 0.017*\"employe\" + 0.017*\"year\" + 0.015*\"write\" + 0.015*\"senior\" + 0.014*\"agreement\" + 0.013*\"prior\" + 0.013*\"lai\" + 0.012*\"decemb\" + 0.012*\"chang\" + 0.011*\"period\" + 0.010*\"classif\" + 0.010*\"notic\" + 0.010*\"termin\" + 0.009*\"end\" + 0.009*\"particular\"\n",
      "-----\n",
      "7 0.050*\"vacat\" + 0.040*\"employe\" + 0.034*\"date\" + 0.027*\"request\" + 0.025*\"period\" + 0.020*\"time\" + 0.019*\"provid\" + 0.017*\"pai\" + 0.015*\"week\" + 0.014*\"year\" + 0.014*\"compani\" + 0.013*\"schedul\" + 0.012*\"copi\" + 0.012*\"end\" + 0.012*\"dai\" + 0.011*\"payrol\" + 0.011*\"articl\" + 0.011*\"calendar\" + 0.011*\"applic\" + 0.010*\"job\"\n",
      "-----\n",
      "8 0.046*\"holidai\" + 0.035*\"dai\" + 0.027*\"work\" + 0.021*\"time\" + 0.016*\"pai\" + 0.015*\"prior\" + 0.015*\"section\" + 0.013*\"period\" + 0.012*\"rate\" + 0.011*\"employe\" + 0.010*\"hour\" + 0.009*\"agreement\" + 0.009*\"compani\" + 0.009*\"disabl\" + 0.008*\"receiv\" + 0.007*\"immedi\" + 0.007*\"plan\" + 0.006*\"regular\" + 0.006*\"avail\" + 0.006*\"thirti\"\n",
      "-----\n",
      "9 0.026*\"employ\" + 0.020*\"oper\" + 0.018*\"employe\" + 0.017*\"transport\" + 0.013*\"safeti\" + 0.012*\"condit\" + 0.011*\"practic\" + 0.010*\"nois\" + 0.010*\"treatment\" + 0.010*\"work\" + 0.009*\"level\" + 0.009*\"aris\" + 0.009*\"safe\" + 0.008*\"chang\" + 0.008*\"avail\" + 0.008*\"obtain\" + 0.007*\"aid\" + 0.007*\"engag\" + 0.007*\"joint\" + 0.007*\"experi\"\n",
      "-----\n",
      "10 0.029*\"employe\" + 0.020*\"time\" + 0.020*\"compani\" + 0.019*\"work\" + 0.017*\"camp\" + 0.013*\"oper\" + 0.013*\"hour\" + 0.013*\"dai\" + 0.012*\"rate\" + 0.010*\"pai\" + 0.008*\"provid\" + 0.008*\"agreement\" + 0.008*\"schedul\" + 0.008*\"safeti\" + 0.007*\"transport\" + 0.006*\"mutual\" + 0.006*\"agre\" + 0.006*\"holidai\" + 0.006*\"requir\" + 0.006*\"condit\"\n",
      "-----\n",
      "11 0.046*\"work\" + 0.033*\"employe\" + 0.033*\"dai\" + 0.030*\"hour\" + 0.026*\"shift\" + 0.024*\"time\" + 0.021*\"pai\" + 0.019*\"rate\" + 0.018*\"schedul\" + 0.018*\"oper\" + 0.016*\"holidai\" + 0.013*\"regular\" + 0.010*\"compani\" + 0.010*\"avail\" + 0.010*\"agreement\" + 0.009*\"period\" + 0.008*\"week\" + 0.008*\"commenc\" + 0.008*\"start\" + 0.007*\"employ\"\n",
      "-----\n",
      "12 0.037*\"employe\" + 0.035*\"work\" + 0.022*\"agreement\" + 0.021*\"hour\" + 0.020*\"dai\" + 0.015*\"time\" + 0.014*\"pai\" + 0.014*\"parti\" + 0.012*\"compani\" + 0.010*\"grievanc\" + 0.010*\"rate\" + 0.009*\"arbitr\" + 0.009*\"oper\" + 0.009*\"senior\" + 0.008*\"regular\" + 0.008*\"write\" + 0.008*\"schedul\" + 0.007*\"foreman\" + 0.007*\"classif\" + 0.007*\"period\"\n",
      "-----\n",
      "13 0.024*\"compani\" + 0.020*\"employe\" + 0.013*\"agreement\" + 0.013*\"oper\" + 0.012*\"work\" + 0.010*\"time\" + 0.010*\"dai\" + 0.009*\"senior\" + 0.009*\"parti\" + 0.008*\"employ\" + 0.008*\"period\" + 0.008*\"plan\" + 0.008*\"month\" + 0.008*\"board\" + 0.008*\"decis\" + 0.007*\"effect\" + 0.007*\"safeti\" + 0.007*\"condit\" + 0.007*\"hour\" + 0.007*\"rate\"\n",
      "-----\n",
      "14 0.034*\"employe\" + 0.016*\"wood\" + 0.014*\"time\" + 0.013*\"employ\" + 0.013*\"scale\" + 0.013*\"oper\" + 0.012*\"weekli\" + 0.012*\"work\" + 0.011*\"disabl\" + 0.011*\"dai\" + 0.010*\"condit\" + 0.010*\"check\" + 0.009*\"accuraci\" + 0.009*\"plan\" + 0.009*\"cut\" + 0.008*\"law\" + 0.008*\"dollar\" + 0.007*\"pai\" + 0.007*\"compani\" + 0.007*\"follow\"\n",
      "-----\n",
      "15 0.036*\"employe\" + 0.030*\"dai\" + 0.019*\"holidai\" + 0.019*\"compani\" + 0.018*\"work\" + 0.016*\"pai\" + 0.016*\"plan\" + 0.015*\"disabl\" + 0.013*\"agreement\" + 0.011*\"benefit\" + 0.011*\"follow\" + 0.011*\"leav\" + 0.008*\"absenc\" + 0.008*\"period\" + 0.008*\"time\" + 0.007*\"insur\" + 0.007*\"elig\" + 0.007*\"provid\" + 0.007*\"employ\" + 0.007*\"lai\"\n",
      "-----\n",
      "16 0.012*\"board\" + 0.011*\"collect\" + 0.011*\"plan\" + 0.011*\"decis\" + 0.010*\"agreement\" + 0.010*\"pension\" + 0.008*\"appendix\" + 0.008*\"master\" + 0.008*\"exclus\" + 0.008*\"polici\" + 0.007*\"forc\" + 0.007*\"term\" + 0.007*\"provis\" + 0.006*\"text\" + 0.006*\"major\" + 0.006*\"avail\" + 0.005*\"request\" + 0.004*\"senior\" + 0.004*\"list\" + 0.004*\"follow\"\n",
      "-----\n",
      "17 0.043*\"senior\" + 0.034*\"employe\" + 0.031*\"work\" + 0.027*\"oper\" + 0.017*\"dai\" + 0.016*\"period\" + 0.015*\"list\" + 0.013*\"compani\" + 0.011*\"lai\" + 0.011*\"time\" + 0.010*\"classif\" + 0.010*\"recal\" + 0.010*\"camp\" + 0.008*\"articl\" + 0.008*\"requir\" + 0.008*\"job\" + 0.008*\"employ\" + 0.007*\"date\" + 0.007*\"commut\" + 0.007*\"divis\"\n",
      "-----\n",
      "18 0.017*\"employe\" + 0.013*\"lai\" + 0.012*\"work\" + 0.011*\"accord\" + 0.010*\"effect\" + 0.010*\"requir\" + 0.009*\"dollar\" + 0.009*\"certif\" + 0.008*\"set\" + 0.008*\"procedur\" + 0.008*\"forth\" + 0.008*\"physician\" + 0.008*\"juli\" + 0.006*\"avail\" + 0.006*\"pursuant\" + 0.006*\"return\" + 0.006*\"medic\" + 0.006*\"coverag\" + 0.006*\"benefit\" + 0.006*\"complet\"\n",
      "-----\n",
      "19 0.046*\"work\" + 0.040*\"employe\" + 0.034*\"time\" + 0.032*\"hour\" + 0.028*\"shift\" + 0.026*\"dai\" + 0.016*\"schedul\" + 0.015*\"regular\" + 0.014*\"senior\" + 0.013*\"compani\" + 0.013*\"avail\" + 0.012*\"rate\" + 0.011*\"agreement\" + 0.011*\"week\" + 0.011*\"employ\" + 0.010*\"oper\" + 0.010*\"pai\" + 0.009*\"commenc\" + 0.008*\"period\" + 0.008*\"chang\"\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "# TODO: Split into \"regular\" LDA run and then the pairwise contract similarities\n",
    "# run (i.e., make two separate pipelines for these two separate tasks)\n",
    "## Load the dictionary and corpus\n",
    "lda_dict_fpath = pl.get_lda_dict_fpath()\n",
    "pl.vprint(\"Loading LDA dictionary from \" + str(lda_dict_fpath))\n",
    "lda_dict = joblib.load(lda_dict_fpath)\n",
    "# Load the combined corpus\n",
    "corpus_fpath = pl.get_lda_corpus_fpath()\n",
    "pl.vprint(\"Loading LDA corpus from \" + str(corpus_fpath))\n",
    "corpus = gensim.corpora.MmCorpus(corpus_fpath)\n",
    "# Construct LDA model\n",
    "if os.path.isfile(pl.get_lda_model_fpath()):\n",
    "    if pl.force_overwrite:\n",
    "        pl.vprint(\"OVERWRITING LDA MODEL\")\n",
    "    else:\n",
    "        input(\"LDA MODEL ALREADY EXISTS. Press Enter to continue and overwrite it \"\n",
    "              + \"or Ctrl+C to kill this script...\")\n",
    "model = launch_lda(corpus, lda_dict)\n",
    "save_lda(pl, model)\n",
    "\n",
    "print_lda(model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 9: Save LDA topics for further analysis"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "outputs": [],
   "source": [
    "# Print and save the LDA topic list to a .txt file\n",
    "import gensim\n",
    "\n",
    "import plutil"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0\n",
      "\n",
      "0.039*\"classif\" + 0.035*\"agreement\" + 0.035*\"job\" + 0.031*\"chang\" + 0.026*\"rate\" + 0.022*\"schedul\" + 0.019*\"wage\" + 0.019*\"establish\" + 0.017*\"attach\" + 0.015*\"employe\"\n",
      "\n",
      "\n",
      "Topic 1\n",
      "\n",
      "0.048*\"dai\" + 0.037*\"work\" + 0.033*\"employe\" + 0.024*\"time\" + 0.021*\"holidai\" + 0.016*\"compani\" + 0.013*\"period\" + 0.012*\"senior\" + 0.011*\"job\" + 0.011*\"oper\"\n",
      "\n",
      "\n",
      "Topic 2\n",
      "\n",
      "0.032*\"holidai\" + 0.030*\"pai\" + 0.027*\"dai\" + 0.024*\"work\" + 0.024*\"employe\" + 0.019*\"time\" + 0.017*\"oper\" + 0.017*\"compani\" + 0.015*\"disabl\" + 0.013*\"agreement\"\n",
      "\n",
      "\n",
      "Topic 3\n",
      "\n",
      "0.026*\"employe\" + 0.017*\"work\" + 0.013*\"compani\" + 0.013*\"provid\" + 0.012*\"benefit\" + 0.011*\"employ\" + 0.011*\"agreement\" + 0.011*\"plan\" + 0.011*\"job\" + 0.011*\"period\"\n",
      "\n",
      "\n",
      "Topic 4\n",
      "\n",
      "0.036*\"work\" + 0.023*\"employe\" + 0.018*\"provid\" + 0.018*\"requir\" + 0.018*\"transport\" + 0.018*\"board\" + 0.016*\"lodg\" + 0.013*\"marshal\" + 0.013*\"camp\" + 0.012*\"agreement\"\n",
      "\n",
      "\n",
      "Topic 5\n",
      "\n",
      "0.037*\"employe\" + 0.036*\"work\" + 0.022*\"dai\" + 0.021*\"employ\" + 0.018*\"compani\" + 0.016*\"oper\" + 0.013*\"hour\" + 0.012*\"camp\" + 0.011*\"time\" + 0.011*\"requir\"\n",
      "\n",
      "\n",
      "Topic 6\n",
      "\n",
      "0.037*\"dai\" + 0.033*\"job\" + 0.028*\"work\" + 0.026*\"parti\" + 0.021*\"desir\" + 0.017*\"employe\" + 0.017*\"year\" + 0.015*\"write\" + 0.015*\"senior\" + 0.014*\"agreement\"\n",
      "\n",
      "\n",
      "Topic 7\n",
      "\n",
      "0.050*\"vacat\" + 0.040*\"employe\" + 0.034*\"date\" + 0.027*\"request\" + 0.025*\"period\" + 0.020*\"time\" + 0.019*\"provid\" + 0.017*\"pai\" + 0.015*\"week\" + 0.014*\"year\"\n",
      "\n",
      "\n",
      "Topic 8\n",
      "\n",
      "0.046*\"holidai\" + 0.035*\"dai\" + 0.027*\"work\" + 0.021*\"time\" + 0.016*\"pai\" + 0.015*\"prior\" + 0.015*\"section\" + 0.013*\"period\" + 0.012*\"rate\" + 0.011*\"employe\"\n",
      "\n",
      "\n",
      "Topic 9\n",
      "\n",
      "0.026*\"employ\" + 0.020*\"oper\" + 0.018*\"employe\" + 0.017*\"transport\" + 0.013*\"safeti\" + 0.012*\"condit\" + 0.011*\"practic\" + 0.010*\"nois\" + 0.010*\"treatment\" + 0.010*\"work\"\n",
      "\n",
      "\n",
      "Topic 10\n",
      "\n",
      "0.029*\"employe\" + 0.020*\"time\" + 0.020*\"compani\" + 0.019*\"work\" + 0.017*\"camp\" + 0.013*\"oper\" + 0.013*\"hour\" + 0.013*\"dai\" + 0.012*\"rate\" + 0.010*\"pai\"\n",
      "\n",
      "\n",
      "Topic 11\n",
      "\n",
      "0.046*\"work\" + 0.033*\"employe\" + 0.033*\"dai\" + 0.030*\"hour\" + 0.026*\"shift\" + 0.024*\"time\" + 0.021*\"pai\" + 0.019*\"rate\" + 0.018*\"schedul\" + 0.018*\"oper\"\n",
      "\n",
      "\n",
      "Topic 12\n",
      "\n",
      "0.037*\"employe\" + 0.035*\"work\" + 0.022*\"agreement\" + 0.021*\"hour\" + 0.020*\"dai\" + 0.015*\"time\" + 0.014*\"pai\" + 0.014*\"parti\" + 0.012*\"compani\" + 0.010*\"grievanc\"\n",
      "\n",
      "\n",
      "Topic 13\n",
      "\n",
      "0.024*\"compani\" + 0.020*\"employe\" + 0.013*\"agreement\" + 0.013*\"oper\" + 0.012*\"work\" + 0.010*\"time\" + 0.010*\"dai\" + 0.009*\"senior\" + 0.009*\"parti\" + 0.008*\"employ\"\n",
      "\n",
      "\n",
      "Topic 14\n",
      "\n",
      "0.034*\"employe\" + 0.016*\"wood\" + 0.014*\"time\" + 0.013*\"employ\" + 0.013*\"scale\" + 0.013*\"oper\" + 0.012*\"weekli\" + 0.012*\"work\" + 0.011*\"disabl\" + 0.011*\"dai\"\n",
      "\n",
      "\n",
      "Topic 15\n",
      "\n",
      "0.036*\"employe\" + 0.030*\"dai\" + 0.019*\"holidai\" + 0.019*\"compani\" + 0.018*\"work\" + 0.016*\"pai\" + 0.016*\"plan\" + 0.015*\"disabl\" + 0.013*\"agreement\" + 0.011*\"benefit\"\n",
      "\n",
      "\n",
      "Topic 16\n",
      "\n",
      "0.012*\"board\" + 0.011*\"collect\" + 0.011*\"plan\" + 0.011*\"decis\" + 0.010*\"agreement\" + 0.010*\"pension\" + 0.008*\"appendix\" + 0.008*\"master\" + 0.008*\"exclus\" + 0.008*\"polici\"\n",
      "\n",
      "\n",
      "Topic 17\n",
      "\n",
      "0.043*\"senior\" + 0.034*\"employe\" + 0.031*\"work\" + 0.027*\"oper\" + 0.017*\"dai\" + 0.016*\"period\" + 0.015*\"list\" + 0.013*\"compani\" + 0.011*\"lai\" + 0.011*\"time\"\n",
      "\n",
      "\n",
      "Topic 18\n",
      "\n",
      "0.017*\"employe\" + 0.013*\"lai\" + 0.012*\"work\" + 0.011*\"accord\" + 0.010*\"effect\" + 0.010*\"requir\" + 0.009*\"dollar\" + 0.009*\"certif\" + 0.008*\"set\" + 0.008*\"procedur\"\n",
      "\n",
      "\n",
      "Topic 19\n",
      "\n",
      "0.046*\"work\" + 0.040*\"employe\" + 0.034*\"time\" + 0.032*\"hour\" + 0.028*\"shift\" + 0.026*\"dai\" + 0.016*\"schedul\" + 0.015*\"regular\" + 0.014*\"senior\" + 0.013*\"compani\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lda_model_fpath = pl.get_lda_model_fpath()\n",
    "lda_model = gensim.models.LdaMulticore.load(lda_model_fpath)\n",
    "all_topics = lda_model.show_topics(num_topics=pl.num_lda_topics)\n",
    "all_topics.sort(key=lambda x: x[0])\n",
    "output_buffer = \"\"\n",
    "for cur_topic in all_topics:\n",
    "    topic_label = \"Topic \" + str(cur_topic[0]) + \"\\n\"\n",
    "    output_buffer += topic_label\n",
    "    print(topic_label)\n",
    "    topic_terms = cur_topic[1] + \"\\n\\n\"\n",
    "    output_buffer += topic_terms\n",
    "    print(topic_terms)\n",
    "plutil.safe_write_to_file(output_buffer, pl.get_lda_output_fpath())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 10: Compute subnorm weights"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}