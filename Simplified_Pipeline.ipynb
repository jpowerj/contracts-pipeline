{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pipeline\n",
    "import plutil\n",
    "\n",
    "from smart_open import s3\n",
    "from tqdm import tqdm\n",
    "\n",
    "from plutil import DocMeta, ArtMeta"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 0: Set up pipeline"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "bucket_name = 'cuecon-textlab'\n",
    "bucket_prefix = 'home/research/corpora/contracts/canadian/txt/'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full output path: ..\\..\\..\\output\\analysis\\2022-12-jeff-python\n",
      "Loading filenames from cuecon-textlab/home/research/corpora/contracts/canadian/txt/...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "44590it [00:23, 1934.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files: 44589; files after language filter (['eng']): 35931\n",
      "35931 filenames loaded (0000102a_eng.txt ... 1498101a_eng.txt)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#pl = pipeline.Pipeline(\"canadian\", canadian_pt_path, lang_list=[\"eng\"],\n",
    "#                       sample_N=100, splitter=\"elliott\")\n",
    "s3_options = {\n",
    "    'bucket_name': 'cuecon-textlab',\n",
    "    'bucket_prefix': 'home/research/corpora/contracts/canadian/txt/'\n",
    "}\n",
    "pl = pipeline.Pipeline(\"canadian\", mode=\"s3\", mode_options=s3_options,\n",
    "                       output_dirname=\"2022-12-jeff-python\", lang_list=[\"eng\"],\n",
    "                       splitter=\"elliott\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "8658"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pl.excluded_fnames)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "('0626406c_fra.txt', '1427601c_fra.txt')"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.excluded_fnames[0], pl.excluded_fnames[-1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 1: Split contract text into individual articles\n",
    "\n",
    "Using the regex method or elliott's method"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\\..\\..\\output\\analysis\\2022-12-jeff-python\\01_artsplit_elliott_pkl\n"
     ]
    }
   ],
   "source": [
    "# Get the paths to the folders where the pkl and json files should be saved\n",
    "artsplit_pkl_path = pl.get_artsplit_output_path(\"pkl\")\n",
    "print(artsplit_pkl_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "import detect_sections_elliott as dse"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def split_contract(contract_text, contract_meta):\n",
    "    arts, headers = dse.detect_sections(contract_text)\n",
    "    # Convert to the dict format for compatibility with regex splitter\n",
    "    # It looks like len(headers) is almost always greater than (often\n",
    "    # like double or triple) len(arts). So for now I'm ignoring headers\n",
    "    art_list = []\n",
    "    for i in range(len(arts)):\n",
    "        cur_art_text = arts[i]\n",
    "        cur_art_meta = ArtMeta(contract_meta.contract_id, contract_meta.lang, i)\n",
    "        art_list.append((cur_art_text, cur_art_meta))\n",
    "    return art_list"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "save_json = False"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 100/35931 [00:06<40:46, 14.65it/s] \n"
     ]
    }
   ],
   "source": [
    "accept_rule = lambda fname: fname.endswith('_eng.txt')\n",
    "# Include key_limit=16 for debugging\n",
    "bucket_iter = s3.iter_bucket(bucket_name, prefix=bucket_prefix,\n",
    "                             accept_key=accept_rule, workers=16, key_limit=100)\n",
    "for fpath, content in tqdm(bucket_iter, total=pl.get_num_docs()):\n",
    "    fname = os.path.basename(fpath)\n",
    "    # First we get the info from the filename\n",
    "    fname_data = plutil.parse_fname(fname)\n",
    "    contract_prefix = fname_data['prefix']\n",
    "    contract_meta = DocMeta(fname_data['id'], fname_data['lang'])\n",
    "    # Now we process the content\n",
    "    #print(fname, len(content))\n",
    "    contract_text = content.decode('utf-8')\n",
    "    art_list = split_contract(contract_text, contract_meta)\n",
    "    # And save the article list as .pkl (for internal use) and .json\n",
    "    # (for human reading)\n",
    "    pkl_fpath = os.path.join(pl.get_artsplit_output_path(\"pkl\"), f\"{contract_prefix}.pkl\")\n",
    "    plutil.safe_to_pickle(art_list, pkl_fpath)\n",
    "    #print(f\"Saved to {pkl_fpath}\")\n",
    "    if save_json:\n",
    "        json_fpath = os.path.join(pl.get_artsplit_output_path(\"json\"), f\"{contract_prefix}.json\")\n",
    "        plutil.safe_to_json(art_list, json_fpath)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Step 2: Parse the articles using spaCy, resolve coreference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "#!pip install multiprocessing_logging"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Python imports\n",
    "import functools\n",
    "import glob\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# 3rd party imports\n",
    "import joblib\n",
    "\n",
    "# Local imports\n",
    "import pipeline\n",
    "from plutil import DocMeta, ArtMeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logger = logging.getLogger()\n",
    "logging.basicConfig(format=\"%(asctime)s : %(levelname)s : %(message)s\", level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# And set it to work with spacy's use of multiprocessing\n",
    "import multiprocessing_logging\n",
    "multiprocessing_logging.install_mp_handler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading spaCy core model\n"
     ]
    }
   ],
   "source": [
    "nlp_eng = pl.get_spacy_model()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def stream_art_lists(test_N=None):\n",
    "    \"\"\"\n",
    "\n",
    "    :param test_N:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    art_data_fpaths = glob.glob(os.path.join(artsplit_pkl_path, \"*.pkl\"))\n",
    "    for fnum, fpath in enumerate(art_data_fpaths):\n",
    "        if test_N is not None and fnum >= test_N:\n",
    "            # We've already yielded the first `test_N` contracts, so terminate\n",
    "            break\n",
    "        # Get the contract info from the fpath\n",
    "        fname = os.path.basename(fpath)\n",
    "        fname_data = plutil.parse_fname(fname)\n",
    "        contract_meta = DocMeta(fname_data['id'], fname_data['lang'])\n",
    "        contract_articles = joblib.load(fpath)\n",
    "        yield contract_articles, contract_meta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#art_data_fpaths = glob.glob(\"../canadian_output/01_artsplit_elliott_json/*.json\")\n",
    "#first_fpath = art_data_fpaths[0]\n",
    "#with open(first_fpath, 'r') as f:\n",
    "#    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def remove_unserializable_results(doc):\n",
    "    doc.user_data = {}\n",
    "    for x in dir(doc._):\n",
    "        if x in ['get', 'set', 'has']: continue\n",
    "        setattr(doc._, x, None)\n",
    "    for token in doc:\n",
    "        for x in dir(token._):\n",
    "            if x in ['get', 'set', 'has']: continue\n",
    "            setattr(token._, x, None)\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_coref_data(doc_obj):\n",
    "    mentions = [\n",
    "        {\n",
    "            \"start\": mention.start_char,\n",
    "            \"end\": mention.end_char,\n",
    "            \"text\": mention.text,\n",
    "            \"resolved\": cluster.main.text,\n",
    "        }\n",
    "        for cluster in doc_obj._.coref_clusters\n",
    "        for mention in cluster.mentions\n",
    "    ]\n",
    "    return mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def transform_texts(nlp, batch_id, batch_tuples, output_dir):\n",
    "    batch_results = []\n",
    "    #print(nlp.pipe_names)\n",
    "    output_fpath = os.path.join(output_dir, f\"{batch_id}.pkl\")\n",
    "    if os.path.isfile(output_fpath):  # return None in case same batch is called again\n",
    "        return None\n",
    "    print(\"Processing batch\", batch_id)\n",
    "    for art_doc, art_meta in nlp.pipe(batch_tuples, as_tuples=True):\n",
    "        # This is the weird part where we now have to change contract_id and art_num\n",
    "        # from being metadata to being attributes of the spacy Doc objects themselves\n",
    "        contract_id = art_meta[\"contract_id\"]\n",
    "        article_num = art_meta[\"article_num\"]\n",
    "        art_doc._.contract_id = contract_id\n",
    "        art_doc._.article_num = article_num\n",
    "        # And now we don't need the meta object anymore, since it's encoded in the Doc itself\n",
    "        # But next we need to get a serializable representation of the detected corefs\n",
    "        art_doc._.coref_list = get_coref_data(art_doc)\n",
    "        # Ok now we can get rid of the original coref attributes that break the data\n",
    "        art_doc = remove_unserializable_results(art_doc)\n",
    "        batch_results.append(art_doc)\n",
    "    # And save the bytes object to file\n",
    "    joblib.dump(batch_results, output_fpath)\n",
    "    print(f\"Saved {len(batch_tuples)} texts to {output_fpath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "def spacy_parse_article(nlp, art_str):\n",
    "    art_doc = nlp(art_str)\n",
    "    ## But next we need to get a serializable representation of the detected corefs\n",
    "    #art_doc._.coref_list = get_coref_data(art_doc)\n",
    "    ## Ok now we can get rid of the original coref attributes that break the data\n",
    "    #art_doc = remove_unserializable_results(art_doc)\n",
    "    return art_doc"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:00, 1803.47it/s]\n"
     ]
    }
   ],
   "source": [
    "spacy_output_path = pl.get_spacy_output_path()\n",
    "contract_iter = tqdm(stream_art_lists())\n",
    "for art_list, contract_meta in contract_iter:\n",
    "    contract_id = contract_meta.get_contract_id()\n",
    "    contract_fname = contract_meta.gen_fname(\"pkl\")\n",
    "    output_fpath = os.path.join(spacy_output_path, contract_fname)\n",
    "    # Check if it's already been processed\n",
    "    if os.path.isfile(output_fpath):\n",
    "        continue\n",
    "    contract_art_list = []\n",
    "    #print(contract_data)\n",
    "    #print(f\"Processing contract {contract_id}\")\n",
    "    num_arts = len(art_list)\n",
    "    contract_iter.set_description(f\"{contract_id} ({num_arts} articles)\")\n",
    "    #pl.vprint(f\"Parsing {len(art_list)} articles\")\n",
    "    for cur_art_text, cur_art_meta in art_list:\n",
    "        cur_art_doc = spacy_parse_article(nlp_eng, cur_art_text)\n",
    "        contract_art_list.append((cur_art_doc, cur_art_meta))\n",
    "    #print(cur_art_doc)\n",
    "    # Save the contract docs\n",
    "    plutil.safe_to_pickle(contract_art_list, output_fpath)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 3: Extract the spaCy dependency parse data (to csv format)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "import process_spacy as pspacy\n",
    "from plutil import DocMeta, ArtMeta\n",
    "\n",
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "nlp_eng = pl.get_spacy_model()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### First extract the statement data (sdata)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "def get_num_contract_parses():\n",
    "    spacy_fpaths = glob.glob(os.path.join(spacy_output_path, \"*.pkl\"))\n",
    "    return len(spacy_fpaths)\n",
    "\n",
    "def stream_contract_parses():\n",
    "    spacy_fpaths = glob.glob(os.path.join(spacy_output_path, \"*.pkl\"))\n",
    "    for cur_fpath in spacy_fpaths:\n",
    "        fname = os.path.basename(cur_fpath)\n",
    "        fname_data = plutil.parse_fname(fname)\n",
    "        contract_id = fname_data['id']\n",
    "        contract_lang = fname_data['lang']\n",
    "        contract_meta = DocMeta(contract_id, contract_lang)\n",
    "        # A list where each element is the spaCy parse of one article within the contract\n",
    "        cur_art_parse_list = joblib.load(cur_fpath)\n",
    "        yield cur_art_parse_list, contract_meta\n",
    "\n",
    "def save_pdata_df(pdata_df, chunk_num=None):\n",
    "    suffix = \"\"\n",
    "    if chunk_num:\n",
    "        suffix = f\"_{str(chunk_num)}\"\n",
    "    pdata_fpath = os.path.join(pl.get_pdata_path(), f\"{pl.get_corpus_name()}_pdata{suffix}.pkl\")\n",
    "    plutil.safe_to_pickle(pdata_df, pdata_fpath)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [04:24<00:00,  2.65s/it]\n"
     ]
    }
   ],
   "source": [
    "num_parses = get_num_contract_parses()\n",
    "for cur_contract_data in tqdm(stream_contract_parses(), total=num_parses):\n",
    "    cur_contract_parses = cur_contract_data[0]\n",
    "    cur_contract_meta = cur_contract_data[1]\n",
    "    cur_contract_id = cur_contract_meta.contract_id\n",
    "    # A list of lists, where each element is the list of statements within an article\n",
    "    contract_statement_lists = []\n",
    "    # The generator gives a list of parses, so run get_statements() on each item\n",
    "    for cur_art_data in cur_contract_parses:\n",
    "        #print(cur_art_data)\n",
    "        cur_art_parse = cur_art_data[0]\n",
    "        cur_art_meta = cur_art_data[1]\n",
    "        cur_art_statements = pspacy.get_statements(cur_art_parse, cur_contract_id, cur_art_meta.art_num)\n",
    "        contract_statement_lists.append(cur_art_statements)\n",
    "    # Now serialize\n",
    "    output_fname = cur_contract_meta.gen_fname(\"pkl\")\n",
    "    output_fpath = os.path.join(pl.get_sdata_output_path(), output_fname)\n",
    "    plutil.safe_to_pickle(contract_statement_lists, output_fpath)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Now we extract the parse data (pdata) for each statement"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting extract_pdata()\n"
     ]
    }
   ],
   "source": [
    "pl.vprint(\"Starting extract_pdata()\")\n",
    "pdata_path = pl.get_pdata_output_path()\n",
    "subject_counts = Counter()\n",
    "subnoun_counts = Counter()\n",
    "modal_counts = Counter()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/35931 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "tuple indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [35]\u001B[0m, in \u001B[0;36m<cell line: 10>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      9\u001B[0m pdata_rows \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m statement_data \u001B[38;5;129;01min\u001B[39;00m tqdm(stream_contract_parses(), total\u001B[38;5;241m=\u001B[39mnum_to_process):\n\u001B[1;32m---> 11\u001B[0m     contract_id \u001B[38;5;241m=\u001B[39m \u001B[43mstatement_data\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcontract_id\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\n\u001B[0;32m     12\u001B[0m     \u001B[38;5;66;03m# Loop over each statement, getting the subject/subject_branch/subject_tag\u001B[39;00m\n\u001B[0;32m     13\u001B[0m     subject \u001B[38;5;241m=\u001B[39m statement_data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msubject\u001B[39m\u001B[38;5;124m'\u001B[39m]\n",
      "\u001B[1;31mTypeError\u001B[0m: tuple indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "# Loop over the items, getting counts but also producing a .csv where each\n",
    "# row is a statement\n",
    "# Num to process is just num_contracts\n",
    "num_to_process = pl.get_num_docs()\n",
    "# Counting individual loop iterations\n",
    "iteration_num = 0\n",
    "# Counting the number of times we save and clear the accumulated data\n",
    "chunk_num = 0\n",
    "pdata_rows = []\n",
    "for statement_data in tqdm(stream_contract_parses(), total=num_to_process):\n",
    "    contract_id = statement_data[\"contract_id\"]\n",
    "    # Loop over each statement, getting the subject/subject_branch/subject_tag\n",
    "    subject = statement_data['subject']\n",
    "    statement_dict = {\n",
    "        'contract_id':contract_id,\n",
    "        'article_num':statement_data['article_num'],\n",
    "        'sentence_num':statement_data['sentence_num'],\n",
    "        'statement_num':statement_data['statement_num'],\n",
    "        'full_sentence':statement_data['full_sentence'],\n",
    "        'full_statement':statement_data['full_statement'],\n",
    "        'subject':statement_data['subject'],\n",
    "        'passive':statement_data['passive'],\n",
    "        'subject_tags':statement_data['subject_tags'],\n",
    "        'subject_branch':statement_data['subject_branch'],\n",
    "        'object_tags':statement_data['object_tags'],\n",
    "        'verb':statement_data['verb'],\n",
    "        'modal':statement_data['modal'],\n",
    "        'md':statement_data['md'],\n",
    "        'neg':statement_data['neg'],\n",
    "        'object_branches':statement_data['object_branches']\n",
    "    }\n",
    "    pdata_rows.append(statement_dict)\n",
    "    subnouns = sorted([subject_branch\n",
    "                       for subject_branch, subject_tag\n",
    "                       in zip(statement_data['subject_branch'], statement_data['subject_tags'])\n",
    "                       if subject_tag.startswith('N')])\n",
    "    subject_counts[subject] += 1\n",
    "    if statement_data['md'] == 1:\n",
    "        modal_counts[subject] += 1\n",
    "    for x in subnouns:\n",
    "        if x != subject:\n",
    "            subnoun_counts[x] += 1\n",
    "    iteration_num = iteration_num + 1\n",
    "    # Print a message and save the statements every 100k\n",
    "    if iteration_num % 100000 == 0:\n",
    "        pl.vprint(f\"Iteration {iteration_num}: Saving [Processing contract {contract_id}]\")\n",
    "        cur_df = pd.DataFrame(pdata_rows)\n",
    "        pl.save_pdata_df(cur_df, chunk_num)\n",
    "        chunk_num = chunk_num + 1\n",
    "        pdata_rows.clear()\n",
    "# Make a Pandas df out of whatever's left in pdata_rows and save it\n",
    "cur_df = pd.DataFrame(pdata_rows)\n",
    "pl.save_pdata_df(cur_df, chunk_num)\n",
    "sub_counts_fpath = os.path.join(pl.get_output_path(), f\"{pl.get_corpus_name()}_subject_counts.pkl\")\n",
    "pd.to_pickle(subject_counts, sub_counts_fpath)\n",
    "modal_counts_fpath = os.path.join(pl.get_output_path(), f\"{pl.get_corpus_name()}_modal_counts.pkl\")\n",
    "pd.to_pickle(modal_counts, modal_counts_fpath)\n",
    "\n",
    "print(\"Most common subjects:\")\n",
    "print(subject_counts.most_common()[:50])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 4: Compute authority measures"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "subject_dict = {\n",
    "    'other':0,'worker':1,'union':2,'firm':3,'manager':4\n",
    "}\n",
    "# And make it reversible\n",
    "for k, v in subject_dict.items():\n",
    "    subject_dict[v] = k\n",
    "\n",
    "snpdata_header = [\n",
    "    \"contract_id\",\"section_num\",\"sentence_num\",\"statement_num\",\"md\",\n",
    "    \"strict_modal\",\"neg\",\"passive\",\"verb\",\"object_branches\",\"full_sentence\"]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def check_neg(statement_row):\n",
    "    return statement_row['neg'] == 'not'\n",
    "\n",
    "def check_strict_modal(statement_row):\n",
    "    strict_modal = False\n",
    "    if statement_row['md']:\n",
    "        strict_modal = statement_row['modal'] in ['shall','must','will']\n",
    "    return strict_modal"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def compute_statement_auth(pl, chunk_num, pdata_chunk_df):\n",
    "    pl.iprint(f\"starting compute_statement_auth()\")\n",
    "    # Uncomment below if the corpus is small enough to be processed all at once\n",
    "    #pdata_df = pl.load_pdata_df()\n",
    "    ### Add some initial columns to the df that will help us when we compute\n",
    "    ### auth measures below\n",
    "    vars_to_keep = [\"contract_id\",\"article_num\",\"sentence_num\",\"statement_num\",\n",
    "                    \"subject\",\"md\",\"verb\",\"passive\",\"full_sentence\"]\n",
    "    ### Soo after this we're grabbing stuff out of auth_df A LOT.\n",
    "    ### so I'm renaming it to just df\n",
    "    df = pdata_chunk_df[vars_to_keep].copy()\n",
    "    # We can save memory by converting some of the ints to bools\n",
    "    df[\"md\"] = df[\"md\"].astype('bool')\n",
    "    df[\"passive\"] = df[\"passive\"].astype('bool')\n",
    "    df[\"subject\"] = df[\"subject\"].str.lower()\n",
    "    df[\"subnorm\"] = df[\"subject\"].apply(pl.normalize_subject)\n",
    "    # Strict modal check. axis=1 means apply row-by-row\n",
    "    df[\"strict_modal\"] = pdata_chunk_df.apply(check_strict_modal,axis=1).astype('bool')\n",
    "    df[\"neg\"] = pdata_chunk_df['neg'].apply(lambda x: x == 'not').astype('bool')\n",
    "\n",
    "    with tqdm(total=13) as pbar:\n",
    "        df['count'] = 1\n",
    "        # permissive modals are may and can\n",
    "        df['permissive_modal'] = (df['md'] & ~df['strict_modal']).astype('bool')\n",
    "        pbar.update(1)\n",
    "\n",
    "        # obligation verbs\n",
    "        df_passive = df['passive']\n",
    "        df['obligation_verb'] = (df_passive &\n",
    "                                 df['verb'].isin(['require', 'expect', 'compel', 'oblige', 'obligate'])).astype('bool')\n",
    "        pbar.update(1)\n",
    "\n",
    "        # constraint verbs\n",
    "        df['constraint_verb'] = (df_passive &\n",
    "                                 df['verb'].isin(['prohibit', 'forbid', 'ban', 'bar', 'restrict', 'proscribe'])).astype('bool')\n",
    "        pbar.update(1)\n",
    "\n",
    "        # permissiion verbs are be allowed, be permitted, and be authorized\n",
    "        df['permission_verb'] =  (df_passive &\n",
    "                                  df['verb'].isin(['allow', 'permit', 'authorize'])).astype('bool')\n",
    "        pbar.update(1)\n",
    "\n",
    "        df_notpassive = ~df_passive\n",
    "        df['entitlement_verb'] =  (df_notpassive &\n",
    "                                   df['verb'].isin(['have', 'receive','retain'])).astype('bool')\n",
    "        pbar.update(1)\n",
    "\n",
    "        df['promise_verb'] = (df_notpassive &\n",
    "                              df['verb'].isin(['agree','promise','commit','recognize',\n",
    "                                               'consent','assent','affirm','assure',\n",
    "                                               'guarantee','insure','ensure','stipulate',\n",
    "                                               'undertake','pledge'])).astype('bool')\n",
    "        pbar.update(1)\n",
    "\n",
    "        pl.dprint(\"Computed up to promise_verb\")\n",
    "\n",
    "        df['special_verb'] = (df['obligation_verb'] | df['constraint_verb'] | df['permission_verb'] | df['entitlement_verb'] | df['promise_verb']).astype('bool')\n",
    "        pbar.update(1)\n",
    "\n",
    "\n",
    "        df['active_verb'] = (df_notpassive & ~df['special_verb']).astype('bool')\n",
    "        pbar.update(1)\n",
    "\n",
    "        #df['verb_type'] = 0 + 1 *df['passive'] + 2*df['obligation_verb'] + 3*df['constraint_verb'] + 4*df['permission_verb'] + 5*df['entitlement_verb']\n",
    "\n",
    "        df_neg = df['neg']\n",
    "        df_notneg = ~df_neg\n",
    "        df['obligation'] = ((df_notneg & df['strict_modal'] & df['active_verb']) |     #positive, strict modal, action verb\n",
    "                            (df_notneg & df['strict_modal'] & df['obligation_verb']) | #positive, strict modal, obligation verb\n",
    "                            (df_notneg & ~df['md'] & df['obligation_verb'])).astype('bool') #positive, non-modal, obligation verb\n",
    "        pbar.update(1)\n",
    "\n",
    "        df['constraint'] = ((df_neg & df['md'] & ~df['obligation_verb']) | # negative, any modal, any verb except obligation verb\n",
    "                            (df_notneg & df['strict_modal'] & df['constraint_verb'])).astype('bool') # positive, strict modal, constraint verb\n",
    "        pbar.update(1)\n",
    "\n",
    "        df['permission'] = ((df_notneg & ( (df['permissive_modal'] & df['active_verb']) |\n",
    "                                           df['permission_verb'])) |\n",
    "                            (df['neg'] & df['constraint_verb'])).astype('bool')\n",
    "        pbar.update(1)\n",
    "\n",
    "\n",
    "        df['entitlement'] = ((df_notneg & df['entitlement_verb']) |\n",
    "                             (df_notneg & df['strict_modal'] & df['passive']) |\n",
    "                             (df_neg & df['obligation_verb'])).astype('bool')\n",
    "        pbar.update(1)\n",
    "    pl.iprint(\"Authority measures computed.\")\n",
    "    pl.save_statement_auth(df, chunk_num)\n",
    "\n",
    "    #grp.replace(np.nan,0,inplace=True)\n",
    "    #pickle_filename = output_prefix + \"-excerpt.pkl\"\n",
    "    #grp.to_pickle(pickle_filename)\n",
    "\n",
    "    #df.to_pickle('df-contract-1.pkl')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### And combine the individual batches to get one file per contract"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "auth_path = pl.get_statement_auth_path()\n",
    "fpath_data = plu.sort_by_suffix(auth_path)\n",
    "new_fname = plu.remove_suffix(auth_path)\n",
    "auth_df = pd.DataFrame()\n",
    "for fnum, fpath in fpath_data:\n",
    "    cur_df = pd.read_pickle(fpath)\n",
    "    auth_df = pd.concat([auth_df,cur_df])\n",
    "# Once combined, save a version without a numeric suffix\n",
    "plutil.safe_to_pickle(auth_df, os.path.join(pl.get_output_path(), new_fname))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 6: Sum authority measures to contract level"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def sum_auth(pl):\n",
    "    \"\"\" Takes the authority measures computed by compute_auth.py and sums them\n",
    "    to the contract level\n",
    "\n",
    "    Requires:\n",
    "    \"\"\"\n",
    "    #breakpoint()\n",
    "    pl.iprint(\"sum_auth()\")\n",
    "    # Loads the df chunks in 04_<corpus>_auth folder produced by compute_auth and sums\n",
    "    # them together to the contract level\n",
    "    summed_df = None\n",
    "    for chunk_num, cur_chunk_df in pl.stream_statement_auth():\n",
    "        # Sum the current chunk\n",
    "        summed_chunk_df = sum_auth_df(pl, cur_chunk_df)\n",
    "        # And concatenate it to the full-data df\n",
    "        if summed_df is None:\n",
    "            summed_df = summed_chunk_df\n",
    "        else:\n",
    "            summed_df = pd.concat([summed_df, summed_chunk_df])\n",
    "    # Now we do a final sum to obtain the (non-chunked) contract-level dataset\n",
    "    final_summed_df = summed_df.groupby([\"contract_id\"]).sum()\n",
    "    final_summed_df.reset_index(inplace=True)\n",
    "    pl.save_authsums_df(summed_df)\n",
    "\n",
    "    sum_csv_fpath = pl.get_authsums_fpath(extension=\"csv\")\n",
    "    sum_df.to_csv(sum_csv_fpath, index=False)\n",
    "    sum_pkl_fpath = pl.get_authsums_fpath(extension=\"pkl\")\n",
    "    sum_df.to_pickle(sum_pkl_fpath)\n",
    "    pl.iprint(\"Saved \" + str(sum_csv_fpath))\n",
    "\n",
    "def sum_auth_df(auth_df):\n",
    "    \"\"\"\n",
    "    Helper function for sum_auth(), which sums up a *single* (chunk) dataframe containing\n",
    "    auth measures, so that they can be combined into one final summed df by sum_auth()\n",
    "    \"\"\"\n",
    "    unique_ids = [\"contract_id\",\"subnorm\"]\n",
    "    grp_df = auth_df.groupby(unique_ids).sum()\n",
    "    grp_df.reset_index(inplace=True)\n",
    "    def conditional_measure(x,subnorm,measure):\n",
    "        # This function returns the actual measure if the subnorm of the row\n",
    "        # is equal to the subnorm argument, and 0.0 otherwise. This makes it\n",
    "        # so that when we \"sum\" up to just contract_id we get just the measure\n",
    "        # for the subnorm of interest\n",
    "        return x[measure] if x[\"subnorm\"]==subnorm else 0.0\n",
    "    subnorms_to_process = pl.subnorm_list\n",
    "    if \"other\" in subnorms_to_process:\n",
    "        subnorms_to_process.remove(\"other\")\n",
    "    for cur_measure in pl.AUTH_MEASURES:\n",
    "        pl.iprint(\"Making cols for \" + cur_measure)\n",
    "        for cur_subnorm in subnorms_to_process:\n",
    "            print(\"Making cols for \" + cur_measure + \" x \" + cur_subnorm)\n",
    "            new_col_name = cur_measure + \"_\" + cur_subnorm\n",
    "            grp_df[new_col_name] = grp_df.apply(conditional_measure,\n",
    "                                                axis=\"columns\",args=(cur_subnorm,cur_measure))\n",
    "    # And now just sum! It's not actually summing anything, just getting rid\n",
    "    # of all the 0.0 cells, for example permission_worker in a row with subnorm firm\n",
    "    sum_df = grp_df.groupby([\"contract_id\"]).sum()\n",
    "    sum_df.reset_index(inplace=True)\n",
    "\n",
    "    # Return it so these summed chunk dfs can be combined into a final summed df\n",
    "    # by sum_auth()\n",
    "    return sum_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 6: Compute corpus-wide summary stats"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import plutil"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Change this list to change which conditionals get counted\n",
    "cond_list = [\"if\",\"in_case\",\"where\",\"were\",\"had\",\"could\",\"unless\",\"should\",\n",
    "             \"as_long_as\",\"so_long_as\",\"provided_that\",\"otherwise\",\"supposing\"]\n",
    "\n",
    "def compute_word_counts(pl):\n",
    "    # There are actually two different possible word counts: the first is just\n",
    "    # the number of tokens in the word-tokenized plaintext. This, to me, is not\n",
    "    # what we want, since (for example) if there are a bunch of spaces between\n",
    "    # each letter this method will produce some huge number of tokens, even\n",
    "    # though each token is actually just one letter. So I think what we want to\n",
    "    # do instead is count the number of words in the full_sentence values for\n",
    "    # each contract.\n",
    "    pl.iprint(\"compute_word_counts()\")\n",
    "    auth_df = pl.get_latest_auth_df()\n",
    "    # First, while it's at the statement level, compute counts\n",
    "    vars_to_keep = [\"contract_id\",\"article_num\",\"sentence_num\",\"statement_num\",\n",
    "                    \"full_sentence\",\"subnorm\"]\n",
    "    wc_df = auth_df[vars_to_keep].copy()\n",
    "    wc_df[\"statement_count\"] = 1\n",
    "    # Subnorm-specific counts\n",
    "    main_subnorms = pl.subnorm_list\n",
    "    main_subnorms.remove(\"other\")\n",
    "    for cur_subnorm in main_subnorms:\n",
    "        count_var = cur_subnorm + \"_count\"\n",
    "        wc_df[count_var] = wc_df[\"subnorm\"] == cur_subnorm\n",
    "        wc_df[count_var] = wc_df[count_var].astype(int)\n",
    "    # Aggregate to sentence level (full_sentence is redundant for different\n",
    "    # statements within the same sentence)\n",
    "    wc_groups = wc_df.groupby(['contract_id','article_num','sentence_num'])\n",
    "    pl.iprint(\"Summing to sentence level\")\n",
    "    agg_dict = {'full_sentence':'first','statement_count':'sum','firm_count':'sum',\n",
    "                'manager_count':'sum','union_count':'sum','worker_count':'sum'}\n",
    "    sent_df = wc_groups.agg(agg_dict)\n",
    "    # slow :(\n",
    "    #main_subnorms = pl.subnorm_list\n",
    "    #main_subnorms.remove(\"other\")\n",
    "    #main_subnorms = [\"firm\"]\n",
    "    #for cur_subnorm in main_subnorms:\n",
    "    #    cur_count_var = cur_subnorm + \"_count\"\n",
    "    #    pl.iprint(\"Computing \" + str(cur_count_var))\n",
    "    #    sent_df[cur_count_var] = auth_groups[\"subnorm\"].agg(lambda x:(x==cur_subnorm).sum())\n",
    "\n",
    "    # Count the words. This regex method is faster than any other, according to\n",
    "    # some stackoverflow post I'm too lazy to find again\n",
    "    wordcount = re.compile(r'\\w+')\n",
    "    def num_tokens(sent_str):\n",
    "        return len(wordcount.findall(sent_str))\n",
    "    pl.iprint(\"Computing num_words\")\n",
    "    sent_df[\"num_words\"] = sent_df[\"full_sentence\"].apply(num_tokens)\n",
    "    pl.iprint(\"Finished computing num_words\")\n",
    "\n",
    "    def count_conditionals(sent_str):\n",
    "        # Uncomment this line if you want to produce a *vector* of counts, i.e.,\n",
    "        # how many times each separate conditional appears in the string\n",
    "        #count_vec = cur_text.count(cur_cond.replace(\"_\",\" \")) for cur_cond in cond_list]\n",
    "        # Otherwise, we just sum all these individual counts up to get one final\n",
    "        # conditional count\n",
    "        return sum([sent_str.count(cur_cond.replace(\"_\",\" \")) for cur_cond in cond_list])\n",
    "    sent_df[\"conditional_count\"] = sent_df[\"full_sentence\"].apply(count_conditionals)\n",
    "    pl.iprint(\"Finished counting conditionals\")\n",
    "\n",
    "    # LIWC counts\n",
    "    def num_matches(reg_str, test_str):\n",
    "        # NaNs, by definition, have zero matches, but Python blows up if we don't\n",
    "        # manually specify...\n",
    "        if str(test_str) == \"nan\":\n",
    "            return 0\n",
    "        num_matches = len(re.findall(reg_str,test_str))\n",
    "        return num_matches\n",
    "    # Loop over each category specified by LIWC_DICT in pipeline.py\n",
    "    for cur_liwc_cat in pl.LIWC_FPATHS:\n",
    "        pl.iprint(\"Counting LIWC category: \" + str(cur_liwc_cat))\n",
    "        # Get the filename of the current LIWC file\n",
    "        cur_liwc_fpath = pl.LIWC_FPATHS[cur_liwc_cat]\n",
    "        cur_liwc_list = plu.stopwords_from_file(cur_liwc_fpath)\n",
    "        cur_regex = plu.list_to_regex(cur_liwc_list)\n",
    "        col_name = \"liwc_\" + str(cur_liwc_cat) + \"_count\"\n",
    "        sent_df[col_name] = sent_df.apply(lambda row: num_matches(cur_regex, row[\"full_sentence\"]), axis=1)\n",
    "\n",
    "    #print(sent_df.head())\n",
    "    #print(sent_df.columns)\n",
    "    pl.iprint(\"Finished LIWC counts\")\n",
    "\n",
    "    # And aggregate to contract level\n",
    "    article_df = sent_df.groupby([\"contract_id\",\"article_num\"]).sum()\n",
    "    contract_df = article_df.groupby([\"contract_id\"]).sum()\n",
    "    contract_df.reset_index(inplace=True)\n",
    "\n",
    "    output_csv_fpath = pl.get_sumstats_fpath(extension=\"csv\")\n",
    "    pl.iprint(\"Saving sumstats to \" + output_csv_fpath)\n",
    "    output_pkl_fpath = pl.get_sumstats_fpath(extension=\"pkl\")\n",
    "    plu.safe_to_csv(contract_df, output_csv_fpath, index=False)\n",
    "    plu.safe_to_pickle(contract_df, output_pkl_fpath)\n",
    "    output_dta_fpath = pl.get_sumstats_fpath(extension=\"dta\")\n",
    "    plu.safe_to_stata(contract_df, output_dta_fpath)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 7: Merge the auth data with all other contract metadata"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "def merge_text_meta(pl):\n",
    "    pl.dprint(\"merge_text_meta()\")\n",
    "    # Merges the full contract-level dataset created by generateContractAuth()\n",
    "    # with the metadata in <corpus_name>_meta.csv\n",
    "    authsums_pkl_fpath = pl.get_authsums_fpath(extension=\"pkl\")\n",
    "    pl.iprint(\"Loading authsums file \" + authsums_pkl_fpath)\n",
    "    authsums_df = pd.read_pickle(authsums_pkl_fpath)\n",
    "    # And make sure the contract_id is the index (unique key)\n",
    "    # Fuck it. Pandas is so finicky I'm just gonna ALWAYS ignore the index. So\n",
    "    # all columns will just be regular columns, and we'll never look at or care\n",
    "    # about the index column. Rrrrrgh.\n",
    "    #contract_df.set_index(plu.UID_CONTRACT,inplace=True)\n",
    "    # Also the contract_id column here sometimes gets loaded as float64, whereas\n",
    "    # in the other file it gets loaded as int64. Again, rrrrrgh.\n",
    "    # TODO: figure out why the hell this happens. 264 contracts have some\n",
    "    # weird hex string as its contract_id\n",
    "    #authsum_df = authsum_df[~authsum_df.contract_id.str.contains('\\xa0')]\n",
    "    authsums_df[\"contract_id\"] = pd.to_numeric(authsums_df[\"contract_id\"],errors='coerce')\n",
    "\n",
    "    meta_fpath = pl.get_meta_fpath()\n",
    "    pl.iprint(\"Loading meta file \" + str(meta_fpath))\n",
    "    meta_df = plu.safe_read_data(meta_fpath)\n",
    "    # contract_id unique key\n",
    "    #meta_df.set_index(plu.UID_CONTRACT,inplace=True)\n",
    "\n",
    "    # Merge on contract_id\n",
    "    merged_df = authsums_df.merge(meta_df, how='inner', on=['contract_id'])\n",
    "    output_csv_fpath = pl.get_merged_meta_fpath(extension=\"csv\")\n",
    "    output_pkl_fpath = pl.get_merged_meta_fpath(extension=\"pkl\")\n",
    "    pl.iprint(\"Saving \" + output_pkl_fpath)\n",
    "    # Make sure contract is unique id\n",
    "    #merged_df.set_index(plu.UID_CONTRACT,inplace=True)\n",
    "    plu.safe_to_csv(merged_df, output_csv_fpath)\n",
    "    plu.safe_to_pickle(merged_df, output_pkl_fpath)\n",
    "    # And update the pipeline to reflect that the basic metadata has been\n",
    "    # successfully merged\n",
    "    pl.last_merge_csv_fpath = output_csv_fpath\n",
    "    pl.last_merge_pkl_fpath = output_pkl_fpath"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}