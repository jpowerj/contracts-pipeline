# Generates data for a topic-by-topic graph (K=16) comparing mean of
# obligation_constraint with mean of permission_entitlement for employers,
# and then another set of graphs comparing this mean for workers
import codecs
import csv
import glob
import os
import pickle

import gensim
from joblib import Parallel, delayed
import numpy as np
import pandas as pd

import pipeline_util as plu

def construct_subnorm_auths(pl):
    pl.iprint("construct_subnorm_auths()")

    ### STEP 1: Turn "obligation","constraint",... into subnorm-specific vars
    ### ("worker_obligation","worker_constraint",...)

    # Multiplies the obligation+permission vars generated by combineBySubject()
    # by the weights for each topic
    # [NOTE: In the process it combines obligation+constraint into obligation
    # and permission+entitlement into permission]

    # And load auths
    auth_fpath = pl.get_auth_fpath()
    #04_canadian_auth.pkl
    auth_df = pd.read_pickle(auth_fpath)
    measures = pl.AUTH_MEASURES
    main_sns = pl.subnorm_list
    main_sns.remove("other")
    for subnorm in main_sns:
        print("Processing " + str(subnorm))
        subnorm_ob = subnorm + "_obligation"
        subnorm_const = subnorm + "_constraint"
        subnorm_perm = subnorm + "_permission"
        subnorm_ent = subnorm + "_entitlement"
        def sn_match(row, subnorm):
            return row["subnorm"] == subnorm
        auth_df[subnorm_ob] = auth_df.apply(lambda row: row["obligation"] if sn_match(row,subnorm) else 0, axis=1)
        print("ob computed")
        auth_df[subnorm_const] = auth_df.apply(lambda row: row["constraint"] if sn_match(row,subnorm) else 0, axis=1)
        print("const computed")
        auth_df[subnorm_perm] = auth_df.apply(lambda row: row["permission"] if sn_match(row,subnorm) else 0, axis=1)
        print("perm computed")
        auth_df[subnorm_ent] = auth_df.apply(lambda row: row["entitlement"] if sn_match(row,subnorm) else 0, axis=1)
        print("ent computed")
    # Now we can delete the "plain" auth measures
    del auth_df["obligation"]
    del auth_df["constraint"]
    del auth_df["permission"]
    del auth_df["entitlement"]
    plu.safe_to_pickle(auth_df, pl.get_snauth_statement_fpath(extension="pkl"))

    # And sum to the article level, since we only have weights for *articles*
    auth_df = auth_df.groupby(["contract_id","article_num"]).sum()
    plu.safe_to_pickle(auth_df, pl.get_subnorm_auth_fpath(extension="pkl"))

    # TODO: Parallelize
    #par_obj = Parallel(n_jobs=7, verbose=5)
    #result_chunks = par_obj(delayed(compute_weighted_auth)(weight_col, lda_model, num_topics) for cur_chunk in corpus_chunks)

    # # Now weight each subnorm-specific auth measure dataset
    # weights_fpath = pl.get_lda_weights_fpath(extension="pkl")
    # weights_df = pd.read_pickle(weights_fpath)
    # pl.iprint("Weights loaded")
    # for subnorm in pl.subnorm_list:
    #     pl.iprint("Processing " + str(subnorm))
    #     # Load the subnorm file
    #     subnorm_fpath = pl.get_subnorm_auth_fpath(subnorm, extension="pkl")
    #     subnorm_df = pd.read_pickle(subnorm_fpath)
    #     for cur_topic_num in range(pl.num_topics):
    #         pl.iprint("Weighting topic " + str(cur_topic_num))
    #         cur_topic_var = "topic_weight_" + str(cur_topic_num)
            
    #         ob_var = subnorm + "_obligation"
    #         con_var = subnorm + "_constraint"
    #         perm_var = subnorm + "_permission"
    #         ent_var = subnorm + "_entitlement"
    #         # Combine the vars
    #         combined_ob_var = subnorm + "_combined_ob"
    #         combined_perm_var = subnorm + "_combined_perm"
    #         subnorm_df[combined_ob_var] = subnorm_df[ob_var] + subnorm_df[con_var]
    #         subnorm_df[combined_perm_var] = subnorm_df[perm_var] + subnorm_df[ent_var]
            
    #         # Multiply by the weights
    #         weighted_ob_var = subnorm + "_weighted_ob_" + str(cur_topic_num)
    #         weighted_perm_var = subnorm + "_weighted_perm_" + str(cur_topic_num)
    #         combined_ob_col = subnorm_df[combined_ob_var]
    #         weight_col = weights_df[cur_topic_var]
    #         print("combined_ob_col")
    #         print(combined_ob_col)
    #         print("weight_col")
    #         print(weight_col)
    #         quit()
    #         subnorm_df[weighted_ob_var] = subnorm_df[combined_ob_var] * weights_df[cur_topic_var]
    #         subnorm_df[weighted_perm_var] = subnorm_df[combined_perm_var] * weights_df[cur_topic_var]

    #         #weight_auth_topic(pl, cur_topic_num, subnorm)
    #     # All topic weights have been multiplied. Save the weighted subnorm file
    #     plu.safe_to_pickle(subnorm_df, pl.get_weighted_snauth_fpath(subnorm))
    
    # # Now sum the statement-level data up to the contract level
    # contract_df = subnorm_df.groupby(["contract_num"])
    # contract_sums = contract_df.sum()
    # vars_to_drop = ["section_num","sentence_num","statement_num"]
    # if "Unnamed: 0" in subj_sums.columns:
    #     vars_to_drop.append("Unnamed: 0")
    # if "Unnamed: 0.1" in subj_sums.columns:
    #     vars_to_drop.append("Unnamed: 0.1")
    # contract_sums.drop(vars_to_drop, axis=1, inplace=True)
    # summed_pkl_fpath = pl.get_summed_weights_fpath(extension="pkl")
    # print("Saving " + summed_pkl_fpath)
    # plu.safe_to_pickle(contract_sums, summed_pkl_fpath)
    # summed_csv_fpath = pl.get_summed_weights_fpath(extension="csv")
    # # NOTE: we *don't* set index=False here because the index is contract_num due
    # # to the groupby(["contract_num"]) above
    # plu.safe_to_csv(contract_sums, summed_csv_fpath)
