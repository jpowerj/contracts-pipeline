# Generates data for a topic-by-topic graph (K=16) comparing mean of
# obligation_constraint with mean of permission_entitlement for employers,
# and then another set of graphs comparing this mean for workers
import codecs
import csv
import glob
import os
import pickle

import gensim
from joblib import Parallel, delayed
import numpy as np
import pandas as pd

import pipeline_util as plu

#def loadDocTerm():
#  # Should be as simple as loading the "canadian_docterm.npz" file
#  # IT WAS NOT THIS SIMPLE :(
#  doc_term_matrix = textacy.fileio.read.read_sparse_csr_matrix("./textacy/canadian_docterm.npz")
#  print(doc_term_matrix.shape)

# [NOTE: Formerly compute_weighted_auth(), changed on 2019-02-19. -JJ]
def weight_subnorm_auths(pl):
    pl.iprint("weight_subnorm_auths()")
    # Multiplies the obligation+permission vars generated by combineBySubject()
    # by the weights for each topic
    # [NOTE: In the process it combines obligation+constraint into obligation
    # and permission+entitlement into permission]
    weights_fpath = pl.get_lda_weights_fpath(extension="pkl")
    weights_df = pd.read_pickle(weights_fpath)
    pl.iprint("Weights loaded")

    # TODO: Parallelize
    #par_obj = Parallel(n_jobs=7, verbose=5)
    #result_chunks = par_obj(delayed(compute_weighted_auth)(weight_col, lda_model, num_topics) for cur_chunk in corpus_chunks)

    # Now weight each subnorm-specific auth measure dataset
    for subnorm in pl.subnorm_list:
        pl.iprint("Processing " + str(subnorm))
        # Load the subnorm file
        subnorm_fpath = pl.get_subnorm_auth_fpath(subnorm, extension="pkl")
        subnorm_df = pd.read_pickle(subnorm_fpath)
        for cur_topic_num in range(pl.num_topics):
            pl.iprint("Weighting topic " + str(cur_topic_num))
            cur_topic_var = "topic_weight_" + str(cur_topic_num)
            
            ob_var = subnorm + "_obligation"
            con_var = subnorm + "_constraint"
            perm_var = subnorm + "_permission"
            ent_var = subnorm + "_entitlement"
            # Combine the vars
            combined_ob_var = subnorm + "_combined_ob"
            combined_perm_var = subnorm + "_combined_perm"
            subnorm_df[combined_ob_var] = subnorm_df[ob_var] + subnorm_df[con_var]
            subnorm_df[combined_perm_var] = subnorm_df[perm_var] + subnorm_df[ent_var]
            
            # Multiply by the weights
            weighted_ob_var = subnorm + "_weighted_ob_" + str(cur_topic_num)
            weighted_perm_var = subnorm + "_weighted_perm_" + str(cur_topic_num)
            combined_ob_col = subnorm_df[combined_ob_var]
            weight_col = weights_df[cur_topic_var]
            print("combined_ob_col")
            print(combined_ob_col)
            print("weight_col")
            print(weight_col)
            quit()
            subnorm_df[weighted_ob_var] = subnorm_df[combined_ob_var] * weights_df[cur_topic_var]
            subnorm_df[weighted_perm_var] = subnorm_df[combined_perm_var] * weights_df[cur_topic_var]

            #weight_auth_topic(pl, cur_topic_num, subnorm)
        # All topic weights have been multiplied. Save the weighted subnorm file
        plu.safe_to_pickle(subnorm_df, pl.get_weighted_snauth_fpath(subnorm))
    
    # Now sum the statement-level data up to the contract level
    contract_df = subnorm_df.groupby(["contract_num"])
    contract_sums = contract_df.sum()
    vars_to_drop = ["section_num","sentence_num","statement_num"]
    if "Unnamed: 0" in subj_sums.columns:
        vars_to_drop.append("Unnamed: 0")
    if "Unnamed: 0.1" in subj_sums.columns:
        vars_to_drop.append("Unnamed: 0.1")
    contract_sums.drop(vars_to_drop, axis=1, inplace=True)
    summed_pkl_fpath = pl.get_summed_weights_fpath(extension="pkl")
    print("Saving " + summed_pkl_fpath)
    plu.safe_to_pickle(contract_sums, summed_pkl_fpath)
    summed_csv_fpath = pl.get_summed_weights_fpath(extension="csv")
    # NOTE: we *don't* set index=False here because the index is contract_num due
    # to the groupby(["contract_num"]) above
    plu.safe_to_csv(contract_sums, summed_csv_fpath)

# if __name__ == "__main__":
#     #corpus_num = 0
    
#     ## First we need to load the dictionary (for converting docs to bags-of-words)
#     dictionary = loadDictionary(CORPUS_NAME + "_dict.pkl")
#     debugPrint("Dictionary loaded")
    
#     ## Now load the LDA model
#     lda = gensim.models.LdaMulticore.load(CORPUS_NAME + "_lda.pkl")
#     debugPrint("LDA model loaded")
    
#     weights_glob = glob.glob(CORPUS_NAME + "_weights*")
#     # If the weights files haven't already been generated
#     if len(weights_glob) == 0:
#         # Compute the weights
#         for corpus_num in range(LDA_NUM_SUBSETS):
#             compute_branch_weights(corpus_num)
    
#     if not os.path.isfile(CORPUS_NAME + "_all_statements_0.csv"):
#         split_statements_file()

#     if not os.path.isfile(CORPUS_NAME + "_statement_weights_0.csv"):
#         for corpus_num in range(LDA_NUM_SUBSETS):
#             merge_weights(corpus_num)
    
#     # Need to merge weights in by subject AND corpus_num
#     for corpus_num in range(LDA_NUM_SUBSETS):
#         for cur_subject in SUBJECTS:
#             # Check if the weights have already been merged for this subject
#             subject_glob = CORPUS_NAME + "_" + cur_subject + "_weights*"
#             if len(subject_glob) == 0:
#                 mergeBySubject(cur_subject,corpus_num)

#     for cur_subject in SUBJECTS:
#         combined_filename = CORPUS_NAME + "_" + cur_subject + "_weights.csv"
#         if not os.path.isfile(combined_filename):
#             combineBySubject(cur_subject)

#     # Check if the employee and employer files have already been generated
#     if not os.path.isfile(CORPUS_NAME + "_employee_weights.csv"):
#         combineTwoSubjects("worker","union","employee")
#     if not os.path.isfile(CORPUS_NAME + "_employee_weights.csv"):
#         combineTwoSubjects("firm","manager","employer")
    
#     for meta_subject in META_SUBJECTS:
#         if not os.path.isfile(CORPUS_NAME + "_" + meta_subject + "_summed.csv"):
#             computeWeightedAuth("employee")
#     # Now to produce lda graphs you can run lda6-generate_authority_graphs.py
#     # or to produce merged text-metadata you can run meta1-merge_text_meta (and
#     # thus start the metadata pipeline)
    
#     # (optional - produces summary statistics)
#     #computeRawCounts("firm")
